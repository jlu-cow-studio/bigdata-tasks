{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bef5d2d-026a-4b41-9ae4-0fa3605c9d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mysql connection\n",
    "import mysql.connector\n",
    "\n",
    "mydb = mysql.connector.connect(\n",
    "  host=\"cowstudio.wayne-lee.cn\",\n",
    "  user=\"cowstudio\",\n",
    "  password=\"cowstudio_2119\",\n",
    "  database=\"cowstudio\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44a24220-0b1d-4961-822f-6cf72cd490a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/24 04:03:38 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/04/24 04:03:39 WARN spark.SparkContext: Please ensure that the number of slots available on your executors is limited by the number of cores to task cpus and not another custom resource. If cores is not the limiting resource then dynamic allocation will not work properly!\n"
     ]
    }
   ],
   "source": [
    "# get spark session, 2g mem per executor\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# set python env\n",
    "os.environ['PYSPARK_PYTHON'] = \"/opt/conda3/envs/lab2/bin/python\"\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"user_item_action_matrix\") \\\n",
    "    .master(\"spark://node01:10077\") \\\n",
    "    .enableHiveSupport()\\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.cores.max\", \"3\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"12\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "428979a5-a303-472c-b7a9-08203bda0a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-24\n",
      "2023-04-23\n"
     ]
    }
   ],
   "source": [
    "# define map functions \n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "today_string = datetime.today().strftime('%Y-%m-%d')\n",
    "history_string = (datetime.today() - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "print(today_string)\n",
    "print(history_string)\n",
    "\n",
    "time_factor = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c6e1cb4-610b-4dde-85c5-d83de221ac3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|      event_type|score|\n",
      "+----------------+-----+\n",
      "|            view|  1.0|\n",
      "|           click|  4.0|\n",
      "|       long_view|  7.0|\n",
      "|add_to_favorites| 20.0|\n",
      "|        purchase| 30.0|\n",
      "|     search_view|  5.0|\n",
      "|    search_click| 10.0|\n",
      "+----------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "event_score_data = [\n",
    "    ('view', 1.0),\n",
    "    ('click', 4.0),\n",
    "    ('long_view', 7.0),\n",
    "    ('add_to_favorites', 20.0),\n",
    "    ('purchase', 30.0),\n",
    "    ('search_view', 5.0),\n",
    "    ('search_click', 10.0)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"event_type\", StringType(), True),\n",
    "    StructField(\"score\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "event_score_df = spark.createDataFrame(event_score_data, schema)\n",
    "event_score_df.show()\n",
    "event_score_df.createOrReplaceTempView(\"event_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d6e5c3f-6034-4b23-866c-c13e88945130",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_action_matrix_schema = StructType([\n",
    "    StructField(\"item_id\", StringType(), True),\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"score\", DoubleType(), True),\n",
    "    StructField(\"date\", StringType(), True)\n",
    "])\n",
    "spark.createDataFrame([],user_item_action_matrix_schema).createOrReplaceTempView(\"user_item_action_matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6583baa3-2b66-48c5-89d0-4dbf58c9a124",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 72:=========================================>            (110 + 2) / 144]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+----------+\n",
      "|item_id|user_id|score|      date|\n",
      "+-------+-------+-----+----------+\n",
      "|    449|    267| 34.0|2023-04-24|\n",
      "|    455|    267| 34.0|2023-04-24|\n",
      "|    461|    267| 34.0|2023-04-24|\n",
      "|    343|    307| 30.0|2023-04-24|\n",
      "|     95|    275| 30.0|2023-04-24|\n",
      "|     78|    212| 30.0|2023-04-24|\n",
      "|     90|    221| 30.0|2023-04-24|\n",
      "|     95|    212| 30.0|2023-04-24|\n",
      "|    100|    275| 30.0|2023-04-24|\n",
      "|     77|    221| 30.0|2023-04-24|\n",
      "|    454|    238| 20.0|2023-04-24|\n",
      "|    457|    238| 20.0|2023-04-24|\n",
      "|     41|    181| 20.0|2023-04-24|\n",
      "|    364|    195| 20.0|2023-04-24|\n",
      "|    464|    238| 20.0|2023-04-24|\n",
      "|     70|    181| 20.0|2023-04-24|\n",
      "|    359|    307| 20.0|2023-04-24|\n",
      "|    232|    307| 20.0|2023-04-24|\n",
      "|    221|    183| 20.0|2023-04-24|\n",
      "|    228|    278| 10.0|2023-04-24|\n",
      "+-------+-------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "user_item_action_matrix = spark.sql(f'''\n",
    "with item_all as(\n",
    "    select\n",
    "        distinct id as item_id\n",
    "    from\n",
    "        item_ods\n",
    "), user_all as(\n",
    "    select\n",
    "        distinct uid as user_id\n",
    "    from\n",
    "        user_ods\n",
    "),history_score as(\n",
    "    select\n",
    "        a.item_id,\n",
    "        b.user_id,\n",
    "        if(c.item_id is null, 0, c.score) as score\n",
    "    from\n",
    "        item_all a\n",
    "    left join\n",
    "        user_all b\n",
    "    left join\n",
    "        user_item_action_matrix c on a.item_id = c.item_id and b.user_id = c.user_id and c.date = '{history_string}'   \n",
    "), today_score as(\n",
    "    select\n",
    "        a.item_id,\n",
    "        b.user_id,\n",
    "        sum(if(d.score is null, 0,d.score)) as score\n",
    "    from\n",
    "        item_all a\n",
    "    left join \n",
    "        user_all b\n",
    "    left join\n",
    "        event_ods c on a.item_id = c.item_id and b.user_id = c.user_id and c.timestamp = '{today_string}'\n",
    "    left join\n",
    "        event_score d on c.event_type = d.event_type\n",
    "    group by\n",
    "        a.item_id,\n",
    "        b.user_id\n",
    "    order by\n",
    "        score desc\n",
    ")\n",
    "select\n",
    "    a.item_id,\n",
    "    b.user_id,\n",
    "    c.score * {time_factor} + d.score as score,\n",
    "    '{today_string}' as date\n",
    "from\n",
    "    item_all a\n",
    "left join\n",
    "    user_all b\n",
    "left join\n",
    "    history_score c on a.item_id = c.item_id and b.user_id = c.user_id\n",
    "left join\n",
    "    today_score d on a.item_id = d.item_id and b.user_id = d.user_id\n",
    "order by\n",
    "    score desc\n",
    "''')\n",
    "user_item_action_matrix.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eccaba37-e300-41f3-aa43-871b06f2c3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "user_item_action_matrix.write.mode(\"overwrite\").partitionBy(\"date\").saveAsTable(\"user_item_action_matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be9a392b-7ad8-431b-b250-6e2f7f043cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+\n",
      "|database|           tableName|isTemporary|\n",
      "+--------+--------------------+-----------+\n",
      "| default|           event_ods|      false|\n",
      "| default|     item_fresh_list|      false|\n",
      "| default|       item_hot_list|      false|\n",
      "| default|            item_ods|      false|\n",
      "| default|          item_order|      false|\n",
      "| default|          item_score|      false|\n",
      "| default|        item_tag_ods|      false|\n",
      "| default|     item_word_count|      false|\n",
      "| default|       item_word_idf|      false|\n",
      "| default|        item_word_tf|      false|\n",
      "| default|     item_word_tfidf|      false|\n",
      "| default|             tag_ods|      false|\n",
      "| default|                test|      false|\n",
      "| default|               test2|      false|\n",
      "| default|user_item_action_...|      false|\n",
      "| default|            user_ods|      false|\n",
      "| default|        user_tag_ods|      false|\n",
      "|        |         event_score|       true|\n",
      "|        |          item_score|       true|\n",
      "|        |user_item_action_...|       true|\n",
      "+--------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "69a07e60-6a63-4832-a57f-490eb12b7f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "mydb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91f2f07-8866-4ba0-b548-074d2fdc7281",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
