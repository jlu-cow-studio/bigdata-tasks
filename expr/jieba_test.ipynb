{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ff7131a-dd59-41e9-98ef-1712a69ae1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "\n",
    "mydb = mysql.connector.connect(\n",
    "  host=\"cowstudio.wayne-lee.cn\",\n",
    "  user=\"cowstudio\",\n",
    "  password=\"cowstudio_2119\",\n",
    "  database=\"cowstudio\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52507062-6d55-4727-b23a-7899e27c317c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/18 10:13:15 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/04/18 10:13:16 WARN spark.SparkContext: Please ensure that the number of slots available on your executors is limited by the number of cores to task cpus and not another custom resource. If cores is not the limiting resource then dynamic allocation will not work properly!\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = \"/opt/conda3/envs/lab2/bin/python\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"jieba_test2\") \\\n",
    "    .master(\"spark://node01:10077\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.cores.max\", \"3\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"12\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96b9ae2d-906d-46ec-bf37-6cdf226478e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import jieba.analyse\n",
    "\n",
    "stop_words_rdd = sc.textFile(\"hdfs:///user/spark_temp/stopwords.dat\")\n",
    "stop_words_set = set(stop_words_rdd.collect())\n",
    "\n",
    "def cut_name_and_desc(item):\n",
    "    id, name, desc = item\n",
    "    name_cut = set(jieba.cut(name))\n",
    "    name_cut_pure = set(jieba.cut(name)) - stop_words_set\n",
    "    desc_cut = set(jieba.cut(desc))\n",
    "    desc_cut_pure = set(jieba.cut(desc)) - stop_words_set\n",
    "    return (id,name_cut,name_cut_pure,desc_cut,desc_cut_pure)\n",
    "\n",
    "def to_count(item):\n",
    "    id,name_cut,name_cut_pure,desc_cut,desc_cut_pure = item\n",
    "    for i in name_cut_pure:\n",
    "        yield ((id, i),1)\n",
    "    # for i in desc_cut_pure:\n",
    "    #     yield (i,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc30e905-5271-49bc-a93f-545cb6c70720",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = mydb.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2afea1a-ca44-4e08-bb40-f231478cebaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.reset()\n",
    "cur.execute(\"SELECT id,name,description from items order by RAND() \")\n",
    "result = cur.fetchall()\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "567c04d7-9846-405c-8687-d7f579bc1ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_items_rdd = sc.parallelize(result)\n",
    "# print(all_items_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49265f4a-414e-4c34-be50-bf90b1793a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "items_cut_rdd = all_items_rdd.map(cut_name_and_desc)\n",
    "# print(rdd2.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8feaf40d-67be-4431-b8c9-8238992e32c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(item):\n",
    "    aa,c = item\n",
    "    a,b = aa\n",
    "    return a,b,c\n",
    "word_count = items_cut_rdd.flatMap(to_count).reduceByKey(lambda a,b:a+b).map(f1)\n",
    "# print(word_count.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eea56d17-b5b9-4a31-827e-05a2e3267a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----------+\n",
      "|item_id|key_word|word_count|\n",
      "+-------+--------+----------+\n",
      "|    319|    农庄|         1|\n",
      "|    458|    喂养|         1|\n",
      "|    181|      牛|         1|\n",
      "|    198|      花|         1|\n",
      "|    454|    岗牛|         1|\n",
      "|    454|  黑白花|         1|\n",
      "|    454|    喂养|         1|\n",
      "|    454|    管理|         1|\n",
      "|    218|      牛|         1|\n",
      "|    218|      药|         1|\n",
      "|    180|    母牛|         1|\n",
      "|    253|      牛|         1|\n",
      "|    262|      牛|         1|\n",
      "|    262|    用药|         1|\n",
      "|    370|    肉牛|         1|\n",
      "|     85|    优质|         1|\n",
      "|    417|    一号|         1|\n",
      "|    355|    优质|         1|\n",
      "|    132|    出售|         1|\n",
      "|    132|    红牛|         1|\n",
      "+-------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "schema = StructType([\n",
    "    StructField(\"item_id\", IntegerType(), True),\n",
    "    StructField(\"key_word\", StringType(), True),\n",
    "    StructField(\"word_count\", IntegerType(), True)\n",
    "])\n",
    "df1 = spark.createDataFrame(word_count, schema)\n",
    "df1.createOrReplaceTempView(\"word_count\")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "176d149d-30b6-4622-8513-ae08dd3aa168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----------+\n",
      "|item_id|key_word|word_count|\n",
      "+-------+--------+----------+\n",
      "|    445|    种苗|         1|\n",
      "|     57|    特产|         1|\n",
      "|    173|    配种|         1|\n",
      "|    449|    岗牛|         1|\n",
      "|     11|      牛|         1|\n",
      "|    228|  驱虫剂|         1|\n",
      "|    358|    肉牛|         1|\n",
      "|    463|    岗牛|         1|\n",
      "|    190|    配种|         1|\n",
      "|    396|  黑白花|         1|\n",
      "|    145|  黑白花|         1|\n",
      "|     40|  牛肉干|         1|\n",
      "|     96|    500g|         1|\n",
      "|    210|荷斯坦种|         1|\n",
      "|    454|    二号|         1|\n",
      "|    253|    奶粉|         1|\n",
      "|    416|    三号|         1|\n",
      "|     13|      黑|         1|\n",
      "|    209|荷斯坦种|         1|\n",
      "|    130|      牛|         1|\n",
      "+-------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql('''\n",
    "    select\n",
    "        item_id,\n",
    "        key_word,\n",
    "        sum(word_count) as word_count\n",
    "    from\n",
    "        word_count\n",
    "    group by \n",
    "        item_id ,key_word\n",
    "    order by\n",
    "        word_count desc\n",
    "''')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb4beed3-be69-4195-b564-0598f2d678d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Hive support is required to CREATE Hive TABLE (AS SELECT);;\n'CreateTable `default`.`item_word_count`, Ignore\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCREATE TABLE IF NOT EXISTS item_word_count (item_id INT, key_word STRING, word_count INT) USING hive\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m result \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124m    insert into table item_word_count select * from word_count\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m'''\u001b[39m)\n\u001b[1;32m      5\u001b[0m result\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/opt/app/spark-3.0.1/python/pyspark/sql/session.py:649\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;129m@ignore_unicode_prefix\u001b[39m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;129m@since\u001b[39m(\u001b[38;5;241m2.0\u001b[39m)\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msql\u001b[39m(\u001b[38;5;28mself\u001b[39m, sqlQuery):\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns a :class:`DataFrame` representing the result of the given query.\u001b[39;00m\n\u001b[1;32m    641\u001b[0m \n\u001b[1;32m    642\u001b[0m \u001b[38;5;124;03m    :return: :class:`DataFrame`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;124;03m    [Row(f1=1, f2=u'row1'), Row(f1=2, f2=u'row2'), Row(f1=3, f2=u'row3')]\u001b[39;00m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 649\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped)\n",
      "File \u001b[0;32m/opt/app/spark-3.0.1/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/app/spark-3.0.1/python/pyspark/sql/utils.py:134\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    130\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconverted\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Hive support is required to CREATE Hive TABLE (AS SELECT);;\n'CreateTable `default`.`item_word_count`, Ignore\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"CREATE TABLE IF NOT EXISTS item_word_count (item_id INT, key_word STRING, word_count INT) USING hive\")\n",
    "result = spark.sql('''\n",
    "    insert into table item_word_count select * from word_count\n",
    "''')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37f52fb-545a-487e-abc7-39b2fdbb937e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
