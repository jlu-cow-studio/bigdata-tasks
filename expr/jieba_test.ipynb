{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ff7131a-dd59-41e9-98ef-1712a69ae1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "\n",
    "mydb = mysql.connector.connect(\n",
    "  host=\"cowstudio.wayne-lee.cn\",\n",
    "  user=\"cowstudio\",\n",
    "  password=\"cowstudio_2119\",\n",
    "  database=\"cowstudio\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52507062-6d55-4727-b23a-7899e27c317c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/18 10:13:15 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/04/18 10:13:16 WARN spark.SparkContext: Please ensure that the number of slots available on your executors is limited by the number of cores to task cpus and not another custom resource. If cores is not the limiting resource then dynamic allocation will not work properly!\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = \"/opt/conda3/envs/lab2/bin/python\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"jieba_test2\") \\\n",
    "    .master(\"spark://node01:10077\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.cores.max\", \"3\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"12\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "96b9ae2d-906d-46ec-bf37-6cdf226478e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import jieba.analyse\n",
    "\n",
    "stop_words_rdd = sc.textFile(\"hdfs:///user/spark_temp/stopwords.dat\")\n",
    "stop_words_set = set(stop_words_rdd.collect())\n",
    "\n",
    "def cut_name_and_desc(item):\n",
    "    id, name, desc = item\n",
    "    name_cut = set(jieba.cut(name))\n",
    "    name_cut_pure = set(jieba.cut(name)) - stop_words_set\n",
    "    desc_cut = set(jieba.cut(desc))\n",
    "    desc_cut_pure = set(jieba.cut(desc)) - stop_words_set\n",
    "    return (id,name_cut,name_cut_pure,desc_cut,desc_cut_pure)\n",
    "\n",
    "def to_count(item):\n",
    "    id,name_cut,name_cut_pure,desc_cut,desc_cut_pure = item\n",
    "    for i in name_cut_pure:\n",
    "        yield ((id, i),1)\n",
    "    for i in desc_cut_pure:\n",
    "        yield ((id,i),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fc30e905-5271-49bc-a93f-545cb6c70720",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = mydb.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f2afea1a-ca44-4e08-bb40-f231478cebaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.reset()\n",
    "cur.execute(\"SELECT id,name,description from items order by RAND() \")\n",
    "result = cur.fetchall()\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "567c04d7-9846-405c-8687-d7f579bc1ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_items_rdd = sc.parallelize(result)\n",
    "# print(all_items_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "49265f4a-414e-4c34-be50-bf90b1793a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "items_cut_rdd = all_items_rdd.map(cut_name_and_desc)\n",
    "# print(rdd2.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8feaf40d-67be-4431-b8c9-8238992e32c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(item):\n",
    "    aa,c = item\n",
    "    a,b = aa\n",
    "    return a,b,c\n",
    "word_count = items_cut_rdd.flatMap(to_count).reduceByKey(lambda a,b:a+b).map(f1)\n",
    "# print(word_count.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eea56d17-b5b9-4a31-827e-05a2e3267a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:===================>                                      (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----------+\n",
      "|item_id|key_word|word_count|\n",
      "+-------+--------+----------+\n",
      "|    406|      牛|         1|\n",
      "|    406|  黑白花|         1|\n",
      "|    406|    毛色|         1|\n",
      "|    406|      强|         1|\n",
      "|    218|      牛|         2|\n",
      "|    218|      药|         1|\n",
      "|    218|    用于|         1|\n",
      "|     86|    细腻|         1|\n",
      "|     86|    选用|         1|\n",
      "|     86|    采用|         1|\n",
      "|     86|    四溢|         1|\n",
      "|     86|    调料|         1|\n",
      "|    255|  牛初乳|         1|\n",
      "|    255|    乳品|         1|\n",
      "|    264|    安徽|         1|\n",
      "|    131|      牛|         2|\n",
      "|    131|    草原|         2|\n",
      "|    131|    整体|         1|\n",
      "|    131|  内蒙古|         1|\n",
      "|    131|    必需|         1|\n",
      "+-------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "schema = StructType([\n",
    "    StructField(\"item_id\", IntegerType(), True),\n",
    "    StructField(\"key_word\", StringType(), True),\n",
    "    StructField(\"word_count\", IntegerType(), True)\n",
    "])\n",
    "df1 = spark.createDataFrame(word_count, schema)\n",
    "df1.createOrReplaceTempView(\"word_count\")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "176d149d-30b6-4622-8513-ae08dd3aa168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----------+----------+\n",
      "|item_id|key_word|count_word|total_word|\n",
      "+-------+--------+----------+----------+\n",
      "|     38|  牛肉干|         2|         5|\n",
      "|    307|    服务|         2|        12|\n",
      "|     46|      黑|         2|        24|\n",
      "|     14|    夏日|         2|        19|\n",
      "|     46|    牛排|         2|        24|\n",
      "|    225|  抗生素|         2|         7|\n",
      "|    307|      牛|         2|        12|\n",
      "|    233|      牛|         2|         5|\n",
      "|     14|      黑|         2|        19|\n",
      "|    248|    药品|         2|        13|\n",
      "|     14|    牧场|         2|        19|\n",
      "|    280|    治疗|         2|         8|\n",
      "|     46|    吉林|         2|        24|\n",
      "|    280|  胃肠炎|         2|         8|\n",
      "|    161|  荷斯坦|         2|        11|\n",
      "|    282|    服务|         2|        11|\n",
      "|    186|  荷斯坦|         2|        11|\n",
      "|    282|    检疫|         2|        11|\n",
      "|     14|    母牛|         2|        19|\n",
      "|    282|      牛|         2|        11|\n",
      "+-------+--------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result = spark.sql('''\n",
    "    with t1 as(\n",
    "        select \n",
    "            item_id,\n",
    "            sum(word_count) as word_count\n",
    "        from\n",
    "            word_count\n",
    "        group by \n",
    "            item_id\n",
    "    )\n",
    "    select\n",
    "        word_count.item_id,\n",
    "        key_word,\n",
    "        sum(word_count.word_count) as count_word,\n",
    "        t1.word_count as total_word\n",
    "    from\n",
    "        word_count\n",
    "    left join\n",
    "        t1 on word_count.item_id = t1.item_id\n",
    "    group by \n",
    "        word_count.item_id ,key_word, t1.word_count\n",
    "    order by\n",
    "        count_word desc\n",
    "''')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cb4beed3-be69-4195-b564-0598f2d678d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"CREATE TABLE IF NOT EXISTS item_word_count (item_id INT, key_word STRING, word_count INT) USING hive\")\n",
    "# result = spark.sql('''\n",
    "#     insert into table item_word_count select * from word_count\n",
    "# ''')\n",
    "# result.show()\n",
    "df1.write.saveAsTable(\"item_word_count_test\", mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d37f52fb-545a-487e-abc7-39b2fdbb937e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----------+\n",
      "|item_id|key_word|word_count|\n",
      "+-------+--------+----------+\n",
      "|    446|  黑白花|         1|\n",
      "|     47|    牛油|         1|\n",
      "|     47|      黑|         1|\n",
      "|     47|      焖|         1|\n",
      "|     16|    优质|         1|\n",
      "|     16|     500|         1|\n",
      "|     16|      黑|         1|\n",
      "|     44|  牛肉干|         1|\n",
      "|     44|    食客|         1|\n",
      "|    400|    红牛|         1|\n",
      "|    445|    岗牛|         1|\n",
      "|    445|  黑白花|         1|\n",
      "|    445|    种苗|         1|\n",
      "|    445|    一号|         1|\n",
      "|    445|    供应|         1|\n",
      "|    334|    安置|         1|\n",
      "|     57|    特产|         1|\n",
      "|    390|    指导|         1|\n",
      "|    390|    喂养|         1|\n",
      "|    342|  牛肉干|         1|\n",
      "|    182|      牛|         1|\n",
      "|    182|  荷斯坦|         1|\n",
      "|    113|    东北|         1|\n",
      "|    114|      牛|         1|\n",
      "|    105|    牛蹄|         1|\n",
      "|    164|    黑牛|         1|\n",
      "|    164|    东北|         1|\n",
      "|    283|      牛|         1|\n",
      "|    283|    配种|         1|\n",
      "|     94|    香脆|         1|\n",
      "+-------+--------+----------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351a5acd-61d7-4a1b-8123-934e0d015ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
