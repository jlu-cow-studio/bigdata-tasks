{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9516382-0ad3-471b-8cb7-a7fc87a7af1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/31 14:07:27 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/31 14:07:28 WARN spark.SparkContext: Please ensure that the number of slots available on your executors is limited by the number of cores to task cpus and not another custom resource. If cores is not the limiting resource then dynamic allocation will not work properly!\n"
     ]
    }
   ],
   "source": [
    "# get spark session, 2g mem per executor\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# set python env\n",
    "os.environ['PYSPARK_PYTHON'] = \"/opt/conda3/envs/lab2/bin/python\"\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CalculateUserScoreAndLevel\") \\\n",
    "    .master(\"spark://node01:10077\") \\\n",
    "    .enableHiveSupport()\\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.cores.max\", \"3\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"12\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a73e4d9-5dae-4c31-af5d-cf908e6ad2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|      event_type|score|\n",
      "+----------------+-----+\n",
      "|            view|  1.0|\n",
      "|           click|  4.0|\n",
      "|       long_view|  7.0|\n",
      "|add_to_favorites| 20.0|\n",
      "|        purchase| 30.0|\n",
      "|     search_view|  5.0|\n",
      "|    search_click| 10.0|\n",
      "+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "event_score_data = [\n",
    "    ('view', 1.0),\n",
    "    ('click', 4.0),\n",
    "    ('long_view', 7.0),\n",
    "    ('add_to_favorites', 20.0),\n",
    "    ('purchase', 30.0),\n",
    "    ('search_view', 5.0),\n",
    "    ('search_click', 10.0)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"event_type\", StringType(), True),\n",
    "    StructField(\"score\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "event_score_df = spark.createDataFrame(event_score_data, schema)\n",
    "event_score_df.show()\n",
    "event_score_df.createOrReplaceTempView(\"event_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3372080-be0a-45c8-a7fa-83ee3a1d1748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define map functions \n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "today_string = datetime.today().strftime('%Y-%m-%d')\n",
    "history_string = (datetime.today() - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "print(today_string)\n",
    "print(history_string)\n",
    "\n",
    "time_factor = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a632b33-9fe3-44f0-8eeb-98643116c120",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'today_string' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 22\u001b[0m\n\u001b[1;32m      1\u001b[0m user_score \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mwith all_users as(\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124m    select\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m        distinct user_id\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m    from\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124m        user_ods\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124m), history_user_score as(\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124m    select\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124m        a.user_id,\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124m        if(b.user_id is null,0,b.score) as score\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124m    from\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124m        all_users a\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124m    left_join\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124m        user_score_and_rec_level b on a.user_id = b.user_id\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124m), today_user_score as(\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124m    select\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124m        a.user_id,\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124m        sum(if(c.score is null,0,c.score)) as score\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124m    from\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124m        all_users a\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124m    left join\u001b[39m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;124m        event_ods b on a.user_id = b.user_id and b.timestamp = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoday_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124m    left join\u001b[39m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124m        event_score c on b.event_type = c.event_type\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124m    group by\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124m        a.user_id\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124m    order by\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124m        score desc\u001b[39m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124m)\u001b[39m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124mselect \u001b[39m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124m    a.user_id,\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124m    b.score + c.score as score,\u001b[39m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoday_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as date\u001b[39m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124mfrom\u001b[39m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124m    all_users a\u001b[39m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124mleft_join\u001b[39m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124m    history_user_score b on a.user_id = b.user_id\u001b[39m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124mleft_join\u001b[39m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124m    today_user_score c on a.user_id = c.user_id\u001b[39m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124morder by\u001b[39m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124m    score desc\u001b[39m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m user_score\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'today_string' is not defined"
     ]
    }
   ],
   "source": [
    "user_score = spark.sql(f\"\"\"\n",
    "with all_users as(\n",
    "    select\n",
    "        distinct user_id\n",
    "    from\n",
    "        user_ods\n",
    "), history_user_score as(\n",
    "    select\n",
    "        a.user_id,\n",
    "        if(b.user_id is null,0,b.score) as score\n",
    "    from\n",
    "        all_users a\n",
    "    left_join\n",
    "        user_score_and_rec_level b on a.user_id = b.user_id\n",
    "), today_user_score as(\n",
    "    select\n",
    "        a.user_id,\n",
    "        sum(if(c.score is null,0,c.score)) as score\n",
    "    from\n",
    "        all_users a\n",
    "    left join\n",
    "        event_ods b on a.user_id = b.user_id and b.timestamp = '{today_string}'\n",
    "    left join\n",
    "        event_score c on b.event_type = c.event_type\n",
    "    group by\n",
    "        a.user_id\n",
    "    order by\n",
    "        score desc\n",
    ")\n",
    "select \n",
    "    a.user_id,\n",
    "    b.score + c.score as score,\n",
    "    '{today_string}' as date\n",
    "from\n",
    "    all_users a\n",
    "left_join\n",
    "    history_user_score b on a.user_id = b.user_id\n",
    "left_join\n",
    "    today_user_score c on a.user_id = c.user_id\n",
    "order by\n",
    "    score desc\n",
    "\"\"\")\n",
    "user_score.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b4f8d5-65b9-4524-853b-2e8116d57026",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fc7488-9391-48a5-ba7b-ed10b84338a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
