{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87322db5-c471-4d71-b0ac-f1296d53b733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get spark session, 2g mem per executor\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# set python env\n",
    "os.environ['PYSPARK_PYTHON'] = \"/opt/conda3/envs/lab2/bin/python\"\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ProcessCattleProdText\") \\\n",
    "    .master(\"spark://node01:10077\") \\\n",
    "    .enableHiveSupport()\\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.cores.max\", \"3\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"12\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df6cd63-2e21-4787-8e1a-57857dc7af24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import jieba and set stop word set\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "\n",
    "stop_words_rdd = sc.textFile(\"hdfs:///user/spark_temp/stopwords.dat\")\n",
    "stop_words_set = set(stop_words_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170f185b-82e6-4481-b155-9f71a6cc628a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define map functions \n",
    "from datetime import datetime\n",
    "\n",
    "date_string = datetime.today().strftime('%Y-%m-%d')\n",
    "# cut item name and description\n",
    "def cut_name_and_desc(item):\n",
    "    id, name, desc = item\n",
    "    name_cut = set(jieba.cut(name))\n",
    "    name_cut_pure = set(jieba.cut(name)) - stop_words_set\n",
    "    desc_cut = set(jieba.cut(desc))\n",
    "    desc_cut_pure = set(jieba.cut(desc)) - stop_words_set\n",
    "    return (id,name_cut,name_cut_pure,desc_cut,desc_cut_pure)\n",
    "\n",
    "# map item's cut list to word count\n",
    "def to_count(item):\n",
    "    id,name_cut,name_cut_pure,desc_cut,desc_cut_pure = item\n",
    "    for i in name_cut_pure:\n",
    "        yield ((id, i),1)\n",
    "    for i in desc_cut_pure:\n",
    "        yield ((id,i),1)\n",
    "        \n",
    "# trnasfer (id, key), count to id,key,count, date\n",
    "def split_key_set_date(item):\n",
    "    key1,count = item\n",
    "    id,key = key1\n",
    "    date = date_string\n",
    "    return id,key,count,date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020afd33-7adb-46c6-9d2f-61d574579390",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_items = spark.sql(\"select id,name,description from item_ods where category='cattle_product'\").rdd\n",
    "print(all_items.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38d6a46-8c1d-4512-8812-e3d3480c48e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut item's name and description\n",
    "cut_items = all_items.map(cut_name_and_desc)\n",
    "print(all_items.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ebab92-d986-4634-af14-c55b36942cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do word count\n",
    "item_word_count = cut_items.flatMap(to_count)\\\n",
    "                    .reduceByKey(lambda a,b:a+b)\\\n",
    "                    .map(split_key_set_date)\n",
    "print(item_word_count.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d28f38-40a8-4b19-9f77-9394f974cb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a table for wordcount\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "schema = StructType([\n",
    "    StructField(\"item_id\", IntegerType(), True),\n",
    "    StructField(\"key_word\", StringType(), True),\n",
    "    StructField(\"word_count\", IntegerType(), True),\n",
    "    StructField(\"date\", StringType(),True)\n",
    "])\n",
    "item_word_count = spark.createDataFrame(item_word_count, schema)\n",
    "item_word_count.createOrReplaceTempView(\"cattle_prod_word_count\")\n",
    "item_word_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74b1484-0e5a-445a-b171-558cb48b6930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将DataFrame写入Hive\n",
    "item_word_count.write.mode(\"overwrite\").partitionBy(\"date\").saveAsTable(\"cattle_prod_word_count\")\n",
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a426e1-e61f-480e-bbe5-6c238e1b1116",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
