{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9516382-0ad3-471b-8cb7-a7fc87a7af1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/31 14:07:27 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/31 14:07:28 WARN spark.SparkContext: Please ensure that the number of slots available on your executors is limited by the number of cores to task cpus and not another custom resource. If cores is not the limiting resource then dynamic allocation will not work properly!\n"
     ]
    }
   ],
   "source": [
    "# get spark session, 2g mem per executor\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# set python env\n",
    "os.environ['PYSPARK_PYTHON'] = \"/opt/conda3/envs/lab2/bin/python\"\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CalculateUserScoreAndLevel\") \\\n",
    "    .master(\"spark://node01:10077\") \\\n",
    "    .enableHiveSupport()\\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.cores.max\", \"3\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"12\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a73e4d9-5dae-4c31-af5d-cf908e6ad2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|      event_type|score|\n",
      "+----------------+-----+\n",
      "|            view|  1.0|\n",
      "|           click|  4.0|\n",
      "|       long_view|  7.0|\n",
      "|add_to_favorites| 20.0|\n",
      "|        purchase| 30.0|\n",
      "|     search_view|  5.0|\n",
      "|    search_click| 10.0|\n",
      "+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "event_score_data = [\n",
    "    ('view', 1.0),\n",
    "    ('click', 4.0),\n",
    "    ('long_view', 7.0),\n",
    "    ('add_to_favorites', 20.0),\n",
    "    ('purchase', 30.0),\n",
    "    ('search_view', 5.0),\n",
    "    ('search_click', 10.0)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"event_type\", StringType(), True),\n",
    "    StructField(\"score\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "event_score_df = spark.createDataFrame(event_score_data, schema)\n",
    "event_score_df.show()\n",
    "event_score_df.createOrReplaceTempView(\"event_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3372080-be0a-45c8-a7fa-83ee3a1d1748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-31\n",
      "2023-05-30\n"
     ]
    }
   ],
   "source": [
    "# define map functions \n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "today_string = datetime.today().strftime('%Y-%m-%d')\n",
    "history_string = (datetime.today() - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "print(today_string)\n",
    "print(history_string)\n",
    "\n",
    "time_factor = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a632b33-9fe3-44f0-8eeb-98643116c120",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "\nno viable alternative at input 'with all_users as(\\n    select\\n        distinct user_id\\n    from\\n        user_ods\\n), history_user_score as(\\n    select\\n        a.user_id,\\n        if(b.user_id is null,0,b.score) as score\\n    from\\n        all_users a\\n    left_join'(line 13, pos 4)\n\n== SQL ==\n\nwith all_users as(\n    select\n        distinct user_id\n    from\n        user_ods\n), history_user_score as(\n    select\n        a.user_id,\n        if(b.user_id is null,0,b.score) as score\n    from\n        all_users a\n    left_join\n----^^^\n        user_score_and_rec_level b on a.user_id = b.user_id\n), today_user_score as(\n    select\n        a.user_id,\n        sum(if(c.score is null,0,c.score)) as score\n    from\n        all_users a\n    left join\n        event_ods b on a.user_id = b.user_id and b.timestamp = '2023-05-31'\n    left join\n        event_score c on b.event_type = c.event_type\n    group by\n        a.user_id\n    order by\n        score desc\n)\nselect \n    a.user_id,\n    b.score + c.score as score,\n    '2023-05-31' as date\nfrom\n    all_users a\nleft_join\n    history_user_score b on a.user_id = b.user_id\nleft_join\n    today_user_score c on a.user_id = c.user_id\norder by\n    score desc\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m user_score \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;43mwith all_users as(\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;43m    select\u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;43m        distinct user_id\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;43m    from\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;43m        user_ods\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;43m), history_user_score as(\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;43m    select\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;43m        a.user_id,\u001b[39;49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;43m        if(b.user_id is null,0,b.score) as score\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;43m    from\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;43m        all_users a\u001b[39;49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;43m    left_join\u001b[39;49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;43m        user_score_and_rec_level b on a.user_id = b.user_id\u001b[39;49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;43m), today_user_score as(\u001b[39;49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;43m    select\u001b[39;49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;43m        a.user_id,\u001b[39;49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;43m        sum(if(c.score is null,0,c.score)) as score\u001b[39;49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;43m    from\u001b[39;49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;43m        all_users a\u001b[39;49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;43m    left join\u001b[39;49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;43m        event_ods b on a.user_id = b.user_id and b.timestamp = \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtoday_string\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;43m    left join\u001b[39;49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;43m        event_score c on b.event_type = c.event_type\u001b[39;49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;43m    group by\u001b[39;49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;43m        a.user_id\u001b[39;49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;43m    order by\u001b[39;49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;43m        score desc\u001b[39;49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;43m)\u001b[39;49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;43mselect \u001b[39;49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;43m    a.user_id,\u001b[39;49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;43m    b.score + c.score as score,\u001b[39;49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtoday_string\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m as date\u001b[39;49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;43mfrom\u001b[39;49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;43m    all_users a\u001b[39;49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;43mleft_join\u001b[39;49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;43m    history_user_score b on a.user_id = b.user_id\u001b[39;49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;43mleft_join\u001b[39;49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;43m    today_user_score c on a.user_id = c.user_id\u001b[39;49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;43morder by\u001b[39;49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;43m    score desc\u001b[39;49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m user_score\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/opt/app/spark-3.0.1/python/pyspark/sql/session.py:649\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;129m@ignore_unicode_prefix\u001b[39m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;129m@since\u001b[39m(\u001b[38;5;241m2.0\u001b[39m)\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msql\u001b[39m(\u001b[38;5;28mself\u001b[39m, sqlQuery):\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns a :class:`DataFrame` representing the result of the given query.\u001b[39;00m\n\u001b[1;32m    641\u001b[0m \n\u001b[1;32m    642\u001b[0m \u001b[38;5;124;03m    :return: :class:`DataFrame`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;124;03m    [Row(f1=1, f2=u'row1'), Row(f1=2, f2=u'row2'), Row(f1=3, f2=u'row3')]\u001b[39;00m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 649\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped)\n",
      "File \u001b[0;32m/opt/app/spark-3.0.1/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/app/spark-3.0.1/python/pyspark/sql/utils.py:134\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    130\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconverted\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mParseException\u001b[0m: \nno viable alternative at input 'with all_users as(\\n    select\\n        distinct user_id\\n    from\\n        user_ods\\n), history_user_score as(\\n    select\\n        a.user_id,\\n        if(b.user_id is null,0,b.score) as score\\n    from\\n        all_users a\\n    left_join'(line 13, pos 4)\n\n== SQL ==\n\nwith all_users as(\n    select\n        distinct user_id\n    from\n        user_ods\n), history_user_score as(\n    select\n        a.user_id,\n        if(b.user_id is null,0,b.score) as score\n    from\n        all_users a\n    left_join\n----^^^\n        user_score_and_rec_level b on a.user_id = b.user_id\n), today_user_score as(\n    select\n        a.user_id,\n        sum(if(c.score is null,0,c.score)) as score\n    from\n        all_users a\n    left join\n        event_ods b on a.user_id = b.user_id and b.timestamp = '2023-05-31'\n    left join\n        event_score c on b.event_type = c.event_type\n    group by\n        a.user_id\n    order by\n        score desc\n)\nselect \n    a.user_id,\n    b.score + c.score as score,\n    '2023-05-31' as date\nfrom\n    all_users a\nleft_join\n    history_user_score b on a.user_id = b.user_id\nleft_join\n    today_user_score c on a.user_id = c.user_id\norder by\n    score desc\n"
     ]
    }
   ],
   "source": [
    "user_score = spark.sql(f\"\"\"\n",
    "with all_users as(\n",
    "    select\n",
    "        distinct user_id\n",
    "    from\n",
    "        user_ods\n",
    "), history_user_score as(\n",
    "    select\n",
    "        a.user_id,\n",
    "        if(b.user_id is null,0,b.score) as score\n",
    "    from\n",
    "        all_users a\n",
    "    left_join\n",
    "        user_score_and_rec_level b on a.user_id = b.user_id\n",
    "), today_user_score as(\n",
    "    select\n",
    "        a.user_id,\n",
    "        sum(if(c.score is null,0,c.score)) as score\n",
    "    from\n",
    "        all_users a\n",
    "    left join\n",
    "        event_ods b on a.user_id = b.user_id and b.timestamp = '{today_string}'\n",
    "    left join\n",
    "        event_score c on b.event_type = c.event_type\n",
    "    group by\n",
    "        a.user_id\n",
    "    order by\n",
    "        score desc\n",
    ")\n",
    "select \n",
    "    a.user_id,\n",
    "    b.score + c.score as score,\n",
    "    '{today_string}' as date\n",
    "from\n",
    "    all_users a\n",
    "left_join\n",
    "    history_user_score b on a.user_id = b.user_id\n",
    "left_join\n",
    "    today_user_score c on a.user_id = c.user_id\n",
    "order by\n",
    "    score desc\n",
    "\"\"\")\n",
    "user_score.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96b4f8d5-65b9-4524-853b-2e8116d57026",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_score_schema = StructType([\n",
    "    StructField(\"item_id\", StringType(), True),\n",
    "    StructField(\"score\", DoubleType(), True),\n",
    "    StructField(\"date\", StringType(), True)\n",
    "])\n",
    "user_score = spark.createDataFrame([],user_score_schema)\n",
    "spark.createDataFrame([],user_score_schema).createOrReplaceTempView(\"user_score_and_rec_level\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0fc7488-9391-48a5-ba7b-ed10b84338a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_score.write.mode(\"overwrite\").partitionBy(\"date\").saveAsTable(\"user_score_and_rec_level\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d23b7900-5ed4-4a69-b31d-90a477ca4ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+\n",
      "|database|           tableName|isTemporary|\n",
      "+--------+--------------------+-----------+\n",
      "| default|     cattle_prod_idf|      false|\n",
      "| default|      cattle_prod_tf|      false|\n",
      "| default|   cattle_prod_tfidf|      false|\n",
      "| default|cattle_prod_word_...|      false|\n",
      "| default|           event_ods|      false|\n",
      "| default|     item_fresh_list|      false|\n",
      "| default|       item_hot_list|      false|\n",
      "| default|            item_ods|      false|\n",
      "| default|          item_order|      false|\n",
      "| default|          item_score|      false|\n",
      "| default|        item_tag_ods|      false|\n",
      "| default|     item_word_count|      false|\n",
      "| default|       item_word_idf|      false|\n",
      "| default|        item_word_tf|      false|\n",
      "| default|     item_word_tfidf|      false|\n",
      "| default|             tag_ods|      false|\n",
      "| default|                test|      false|\n",
      "| default|               test2|      false|\n",
      "| default|user_item_action_...|      false|\n",
      "| default|            user_ods|      false|\n",
      "+--------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b57d07-f56e-46d6-91fa-072d84a92913",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ceef35-2b42-4261-884b-299deb1ec680",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
