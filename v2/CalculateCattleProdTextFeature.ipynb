{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0f65770-7652-4426-8562-79977913da8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/01 10:05:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/01 10:05:12 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/06/01 10:05:12 WARN util.Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/06/01 10:05:12 WARN util.Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "23/06/01 10:05:12 WARN util.Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "23/06/01 10:05:12 WARN spark.SparkContext: Please ensure that the number of slots available on your executors is limited by the number of cores to task cpus and not another custom resource. If cores is not the limiting resource then dynamic allocation will not work properly!\n"
     ]
    }
   ],
   "source": [
    "# get spark session, 2g mem per executor\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# set python env\n",
    "os.environ['PYSPARK_PYTHON'] = \"/opt/conda3/envs/lab2/bin/python\"\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CalculateCattleProdTextFeature\") \\\n",
    "    .master(\"spark://node01:10077\") \\\n",
    "    .enableHiveSupport()\\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.cores.max\", \"1\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"12\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7232195b-bc9e-4b21-8642-b97e0d2d438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "date_string = datetime.today().strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a41e55d-5503-4663-8268-aff1e5da1576",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/01 10:05:31 WARN session.SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cattle_prod_item_text_feature = spark.sql(f'''\n",
    "with key_words as(\n",
    "    select\n",
    "        key_word\n",
    "    from\n",
    "        cattle_prod_tfidf\n",
    "    where\n",
    "        date = '{date_string}'\n",
    "    group by\n",
    "        key_word\n",
    "    order by\n",
    "        sum(tfidf) desc\n",
    "    limit 30\n",
    "), all_items as(\n",
    "    select\n",
    "        distinct id as item_id\n",
    "    from\n",
    "        item_ods\n",
    "    where\n",
    "        date = '{date_string}'\n",
    "    and\n",
    "        category = 'twitte'\n",
    "), item_words_cross(\n",
    "    select\n",
    "        item_id,\n",
    "        key_word\n",
    "    from\n",
    "        key_words,\n",
    "        all_items\n",
    ")\n",
    "select\n",
    "    a.item_id,\n",
    "    a.key_word,\n",
    "    if(b.tfidf is null,0,b.tfidf) as val,\n",
    "    '{date_string}' as date\n",
    "from\n",
    "    item_words_cross a\n",
    "left join\n",
    "    cattle_prod_tfidf b on a.key_word = b.key_word and a.item_id = b.item_id\n",
    "''')\n",
    "cattle_prod_item_text_feature.write.mode(\"overwrite\").partitionBy(\"date\").saveAsTable(\"cattle_prod_item_text_feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9e6ea0b-e82a-4db3-826c-e8e15a66d5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, IntegerType, StructType, StructField, DoubleType\n",
    "\n",
    "# 定义schema\n",
    "schema = StructType([\n",
    "    StructField(\"user_id\", IntegerType(), True),\n",
    "    StructField(\"key_word\", StringType(), True),\n",
    "    StructField(\"val\", DoubleType(), True),\n",
    "    StructField(\"date\", StringType(), True)\n",
    "])\n",
    "\n",
    "# 创建一个空的DataFrame\n",
    "spark.createDataFrame([],schema).write.mode(\"overwrite\").partitionBy(\"date\").saveAsTable(\"cattle_prod_user_text_feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ac09f84-dddd-48cc-a65a-80d4c5c9e480",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "RDD is empty",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m rdd_title_user_feature \u001b[38;5;241m=\u001b[39m table_a\u001b[38;5;241m.\u001b[39mfilter(table_a\u001b[38;5;241m.\u001b[39mdate \u001b[38;5;241m==\u001b[39m date_string)\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitem_id\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mrdd\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m row: MatrixEntry(row\u001b[38;5;241m.\u001b[39muser_id, row\u001b[38;5;241m.\u001b[39mitem_id, row\u001b[38;5;241m.\u001b[39mval))\n\u001b[1;32m      6\u001b[0m rdd_title_item_feature \u001b[38;5;241m=\u001b[39m table_b\u001b[38;5;241m.\u001b[39mfilter(table_b\u001b[38;5;241m.\u001b[39mdate \u001b[38;5;241m==\u001b[39m date_string)\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitem_id\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkey_word\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mrdd\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m row:MatrixEntry(row\u001b[38;5;241m.\u001b[39mitem_id, row\u001b[38;5;241m.\u001b[39mkey_word, row\u001b[38;5;241m.\u001b[39mval))\n\u001b[0;32m----> 7\u001b[0m mat_user_item \u001b[38;5;241m=\u001b[39m \u001b[43mCoordinateMatrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrdd_title_user_feature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m mat_item_word \u001b[38;5;241m=\u001b[39m CoordinateMatrix(rdd_title_item_feature)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#用户-物品， 物品-关键字矩阵相乘\u001b[39;00m\n",
      "File \u001b[0;32m/opt/app/spark-3.0.1/python/pyspark/mllib/linalg/distributed.py:806\u001b[0m, in \u001b[0;36mCoordinateMatrix.__init__\u001b[0;34m(self, entries, numRows, numCols)\u001b[0m\n\u001b[1;32m    799\u001b[0m     entries \u001b[38;5;241m=\u001b[39m entries\u001b[38;5;241m.\u001b[39mmap(_convert_to_matrix_entry)\n\u001b[1;32m    800\u001b[0m     \u001b[38;5;66;03m# We use DataFrames for serialization of MatrixEntry entries\u001b[39;00m\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;66;03m# from Python, so first convert the RDD to a DataFrame on\u001b[39;00m\n\u001b[1;32m    802\u001b[0m     \u001b[38;5;66;03m# this side. This will convert each MatrixEntry to a Row\u001b[39;00m\n\u001b[1;32m    803\u001b[0m     \u001b[38;5;66;03m# containing the 'i', 'j', and 'value' values, which can\u001b[39;00m\n\u001b[1;32m    804\u001b[0m     \u001b[38;5;66;03m# each be easily serialized. We will convert back to\u001b[39;00m\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;66;03m# MatrixEntry inputs on the Scala side.\u001b[39;00m\n\u001b[0;32m--> 806\u001b[0m     java_matrix \u001b[38;5;241m=\u001b[39m callMLlibFunc(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreateCoordinateMatrix\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mentries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoDF\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    807\u001b[0m                                 long(numRows), long(numCols))\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(entries, JavaObject)\n\u001b[1;32m    809\u001b[0m       \u001b[38;5;129;01mand\u001b[39;00m entries\u001b[38;5;241m.\u001b[39mgetClass()\u001b[38;5;241m.\u001b[39mgetSimpleName() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCoordinateMatrix\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    810\u001b[0m     java_matrix \u001b[38;5;241m=\u001b[39m entries\n",
      "File \u001b[0;32m/opt/app/spark-3.0.1/python/pyspark/sql/session.py:61\u001b[0m, in \u001b[0;36m_monkey_patch_RDD.<locals>.toDF\u001b[0;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtoDF\u001b[39m(\u001b[38;5;28mself\u001b[39m, schema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sampleRatio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    Converts current :class:`RDD` into a :class:`DataFrame`\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m    [Row(name=u'Alice', age=1)]\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampleRatio\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/app/spark-3.0.1/python/pyspark/sql/session.py:605\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pandas\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m    602\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(\n\u001b[1;32m    604\u001b[0m         data, schema, samplingRatio, verifySchema)\n\u001b[0;32m--> 605\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/app/spark-3.0.1/python/pyspark/sql/session.py:628\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    625\u001b[0m     prepare \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m obj: obj\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, RDD):\n\u001b[0;32m--> 628\u001b[0m     rdd, schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromRDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    630\u001b[0m     rdd, schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromLocal(\u001b[38;5;28mmap\u001b[39m(prepare, data), schema)\n",
      "File \u001b[0;32m/opt/app/spark-3.0.1/python/pyspark/sql/session.py:425\u001b[0m, in \u001b[0;36mSparkSession._createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;124;03mCreate an RDD for DataFrame from an existing RDD, returns the RDD and schema.\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 425\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inferSchema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    426\u001b[0m     converter \u001b[38;5;241m=\u001b[39m _create_converter(struct)\n\u001b[1;32m    427\u001b[0m     rdd \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmap(converter)\n",
      "File \u001b[0;32m/opt/app/spark-3.0.1/python/pyspark/sql/session.py:396\u001b[0m, in \u001b[0;36mSparkSession._inferSchema\u001b[0;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_inferSchema\u001b[39m(\u001b[38;5;28mself\u001b[39m, rdd, samplingRatio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;124;03m    Infer schema from an RDD of Row or tuple.\u001b[39;00m\n\u001b[1;32m    391\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;124;03m    :return: :class:`pyspark.sql.types.StructType`\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 396\u001b[0m     first \u001b[38;5;241m=\u001b[39m \u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfirst\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m first:\n\u001b[1;32m    398\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe first row in RDD is empty, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    399\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcan not infer schema\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/app/spark-3.0.1/python/pyspark/rdd.py:1467\u001b[0m, in \u001b[0;36mRDD.first\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rs:\n\u001b[1;32m   1466\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 1467\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD is empty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: RDD is empty"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg.distributed import CoordinateMatrix, MatrixEntry\n",
    "#创建矩阵\n",
    "table_a = spark.table(\"cattle_prod_user_action_matrix\")\n",
    "table_b = spark.table(\"cattle_prod_item_text_feature\")\n",
    "rdd_title_user_feature = table_a.filter(table_a.date == date_string).select('user_id','item_id','val').rdd.map(lambda row: MatrixEntry(row.user_id, row.item_id, row.val))\n",
    "rdd_title_item_feature = table_b.filter(table_b.date == date_string).select('item_id','key_word','val').rdd.map(lambda row:MatrixEntry(row.item_id, row.key_word, row.val))\n",
    "mat_user_item = CoordinateMatrix(rdd_title_user_feature)\n",
    "mat_item_word = CoordinateMatrix(rdd_title_item_feature)\n",
    "\n",
    "#用户-物品， 物品-关键字矩阵相乘\n",
    "mat_user_item = mat_user_item.toBlockMatrix()\n",
    "mat_item_word = mat_item_word.toBlockMatrix()\n",
    "result = mat_user_item.multiply(mat_item_word)\n",
    "\n",
    "#用户-关键字 特征存储\n",
    "cattle_prod_user_text_feature = result.toCoordinateMatrix().entries.map(lambda e: (e.i,e.j.e.value)).toDF([\"user_id\",\"key_word\",\"val\"])\n",
    "cattle_prod_user_text_feature.write.mode(\"overwrite\").partitionBy(\"date\").saveAsTable(\"cattle_prod_user_text_feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b74246-b534-4ddc-b129-a89f9d8abd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcd92ee-58ff-438b-88a7-e3a1ae44622a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
