{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c3d201-6952-41df-823c-e7a592c24ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get spark session, 2g mem per executor\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# set python env\n",
    "os.environ['PYSPARK_PYTHON'] = \"/opt/conda3/envs/lab2/bin/python\"\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CattleProdGenerateCFRecommendation\") \\\n",
    "    .master(\"spark://node01:10077\") \\\n",
    "    .enableHiveSupport()\\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.cores.max\", \"1\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"12\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692fdc47-6875-4410-a9a7-6827919850d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.mllib.linalg.distributed import IndexedRow, IndexedRowMatrix\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 读取Hive表\n",
    "user_factors = spark.table(\"cattle_prod_user_fact_matrix\")\n",
    "item_factors = spark.table(\"cattle_prod_item_fact_matrix\")\n",
    "\n",
    "# 转换因子数据到向量格式\n",
    "user_factors_vec = user_factors.rdd.map(lambda row: IndexedRow(row.user_id, DenseVector(row[1:])))\n",
    "item_factors_vec = item_factors.rdd.map(lambda row: IndexedRow(row.item_id, DenseVector(row[1:])))\n",
    "\n",
    "# 创建IndexedRowMatrix对象\n",
    "user_matrix = IndexedRowMatrix(user_factors_vec)\n",
    "item_matrix = IndexedRowMatrix(item_factors_vec)\n",
    "\n",
    "# 计算矩阵乘法\n",
    "product_matrix = user_matrix.multiply(item_matrix.toBlockMatrix().transpose().toIndexedRowMatrix())\n",
    "\n",
    "# 将结果转换回DataFrame\n",
    "result_df = product_matrix.rows.toDF([\"user_id\", \"features\"])\n",
    "result_df = result_df.select(F.col(\"user_id\"), F.posexplode(F.col(\"features\"))).selectExpr(\"user_id\", \"pos as item_id\", \"col as val\")\n",
    "\n",
    "# 存储结果回Hive\n",
    "result_df.createOrReplaceTempView(\"cattle_prod_user_item_product\")\n",
    "spark.sql(f'''\n",
    "select\n",
    "    user_id,\n",
    "    collect_list(item_id) over (partition by user_id order by val desc) as rec_list\n",
    "from\n",
    "    cattle_prod_user_item_product\n",
    "''').write.mode(\"overwrite\").partitionBy(\"date\").saveAsTable(\"cattle_prod_cf_based_rec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5580182f-dd0d-4df7-9d78-613426d89d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15118fe-94fd-45e5-83c9-81051baff5dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37060076-452a-4c1c-b732-eedb29840455",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
