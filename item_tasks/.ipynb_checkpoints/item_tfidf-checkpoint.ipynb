{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "371b2f3e-b0ff-40b9-87b3-d8e75e1750da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mysql connection\n",
    "import mysql.connector\n",
    "\n",
    "mydb = mysql.connector.connect(\n",
    "  host=\"cowstudio.wayne-lee.cn\",\n",
    "  user=\"cowstudio\",\n",
    "  password=\"cowstudio_2119\",\n",
    "  database=\"cowstudio\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8033e6f-f283-4190-8e7a-cab88023fe04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/21 14:51:39 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/04/21 14:51:41 WARN spark.SparkContext: Please ensure that the number of slots available on your executors is limited by the number of cores to task cpus and not another custom resource. If cores is not the limiting resource then dynamic allocation will not work properly!\n"
     ]
    }
   ],
   "source": [
    "# get spark session, 2g mem per executor\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# set python env\n",
    "os.environ['PYSPARK_PYTHON'] = \"/opt/conda3/envs/lab2/bin/python\"\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"tiem_tfidf\") \\\n",
    "    .master(\"spark://node01:10077\") \\\n",
    "    .enableHiveSupport()\\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.cores.max\", \"3\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"12\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55015174-77c9-47ab-a7c9-2e847da24a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# import jieba and set stop word set\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "\n",
    "stop_words_rdd = sc.textFile(\"hdfs:///user/spark_temp/stopwords.dat\")\n",
    "stop_words_set = set(stop_words_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfe22ea0-d61d-4ece-a0ea-dbce51d89c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define map functions \n",
    "from datetime import datetime\n",
    "\n",
    "date_string = datetime.today().strftime('%Y-%m-%d')\n",
    "# cut item name and description\n",
    "def cut_name_and_desc(item):\n",
    "    id, name, desc = item\n",
    "    name_cut = set(jieba.cut(name))\n",
    "    name_cut_pure = set(jieba.cut(name)) - stop_words_set\n",
    "    desc_cut = set(jieba.cut(desc))\n",
    "    desc_cut_pure = set(jieba.cut(desc)) - stop_words_set\n",
    "    return (id,name_cut,name_cut_pure,desc_cut,desc_cut_pure)\n",
    "\n",
    "# map item's cut list to word count\n",
    "def to_count(item):\n",
    "    id,name_cut,name_cut_pure,desc_cut,desc_cut_pure = item\n",
    "    for i in name_cut_pure:\n",
    "        yield ((id, i),1)\n",
    "    for i in desc_cut_pure:\n",
    "        yield ((id,i),1)\n",
    "        \n",
    "# trnasfer (id, key), count to id,key,count, date\n",
    "def split_key_set_date(item):\n",
    "    key1,count = item\n",
    "    id,key = key1\n",
    "    date = date_string\n",
    "    return id,key,count,date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43819cd8-4a35-49eb-9119-3f7f537e4a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# get all item data from db, set result to an RDD\n",
    "cur = mydb.cursor()\n",
    "cur.execute(\"SELECT id,name,description from items order by RAND() \")\n",
    "result = cur.fetchall()\n",
    "all_items = sc.parallelize(result)\n",
    "print(all_items.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b47b727-0feb-468b-bf07-ec92ea0fcce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "433\n"
     ]
    }
   ],
   "source": [
    "all_items = spark.sql(\"select id,name,description from item_ods\").rdd\n",
    "print(all_items.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e0e81cd-90a6-4e3a-a1a8-b771cabc492c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "433\n"
     ]
    }
   ],
   "source": [
    "# cut item's name and description\n",
    "cut_items = all_items.map(cut_name_and_desc)\n",
    "print(all_items.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b665cdb-b2b4-4532-9627-70b63e9a1308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# do word count\n",
    "item_word_count = cut_items.flatMap(to_count)\\\n",
    "                    .reduceByKey(lambda a,b:a+b)\\\n",
    "                    .map(split_key_set_date)\n",
    "print(item_word_count.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84b80ed0-1322-4825-a291-b74594dab776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----------+----------+\n",
      "|item_id|key_word|word_count|      date|\n",
      "+-------+--------+----------+----------+\n",
      "|    227|    牛用|         1|2023-04-21|\n",
      "|    227|    饲养|         1|2023-04-21|\n",
      "|    227|      牛|         1|2023-04-21|\n",
      "|    227|多种营养|         1|2023-04-21|\n",
      "|    227|    产地|         1|2023-04-21|\n",
      "|    227|    物质|         1|2023-04-21|\n",
      "|    227|    富含|         1|2023-04-21|\n",
      "|    227|    日常|         1|2023-04-21|\n",
      "|    228|    清凉|         1|2023-04-21|\n",
      "|    228|  驱虫剂|         1|2023-04-21|\n",
      "|    228|    驱虫|         1|2023-04-21|\n",
      "|    228|    降温|         1|2023-04-21|\n",
      "|    228|    蚊虫|         1|2023-04-21|\n",
      "|    228|    叮咬|         1|2023-04-21|\n",
      "|    228|    用于|         1|2023-04-21|\n",
      "|    229|  止咳药|         1|2023-04-21|\n",
      "|    229|    咳喘|         1|2023-04-21|\n",
      "|    229|    牛用|         1|2023-04-21|\n",
      "|    229|    咳嗽|         1|2023-04-21|\n",
      "|    229|      牛|         1|2023-04-21|\n",
      "+-------+--------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a table for wordcount\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "schema = StructType([\n",
    "    StructField(\"item_id\", IntegerType(), True),\n",
    "    StructField(\"key_word\", StringType(), True),\n",
    "    StructField(\"word_count\", IntegerType(), True),\n",
    "    StructField(\"date\", StringType(),True)\n",
    "])\n",
    "item_word_count = spark.createDataFrame(item_word_count, schema)\n",
    "item_word_count.createOrReplaceTempView(\"item_word_count\")\n",
    "item_word_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c1fb03a-6926-4482-920c-f48a520163fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/21 14:55:46 WARN session.SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+-----------+\n",
      "|database|      tableName|isTemporary|\n",
      "+--------+---------------+-----------+\n",
      "| default|       item_ods|      false|\n",
      "| default|item_word_count|      false|\n",
      "| default|        tag_ods|      false|\n",
      "| default|           test|      false|\n",
      "| default|          test2|      false|\n",
      "| default|       user_ods|      false|\n",
      "|        |item_word_count|       true|\n",
      "+--------+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 将DataFrame写入MySQL\n",
    "item_word_count.write.mode(\"overwrite\").partitionBy(\"date\").saveAsTable(\"item_word_count\")\n",
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b221b292-6b30-4d47-b21f-77a05cf6ece7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+------------+------------------+----------+\n",
      "|key_word|item_num_has_word|item_num_all|               idf|      date|\n",
      "+--------+-----------------+------------+------------------+----------+\n",
      "|    吉利|                1|         433|2.6364878963533656|2023-04-21|\n",
      "|    每日|                1|         433|2.6364878963533656|2023-04-21|\n",
      "|    后代|                1|         433|2.6364878963533656|2023-04-21|\n",
      "|      嘴|                1|         433|2.6364878963533656|2023-04-21|\n",
      "|    山地|                1|         433|2.6364878963533656|2023-04-21|\n",
      "|    书籍|                1|         433|2.6364878963533656|2023-04-21|\n",
      "|    弹牙|                1|         433|2.6364878963533656|2023-04-21|\n",
      "|产品质量|                1|         433|2.6364878963533656|2023-04-21|\n",
      "|    强化|                1|         433|2.6364878963533656|2023-04-21|\n",
      "|    优秀|                1|         433|2.6364878963533656|2023-04-21|\n",
      "|    感染|                1|         433|2.6364878963533656|2023-04-21|\n",
      "|    储藏|                1|         433|2.6364878963533656|2023-04-21|\n",
      "|    成分|                1|         433|2.6364878963533656|2023-04-21|\n",
      "|  养牛场|                1|         433|2.6364878963533656|2023-04-21|\n",
      "|    或伴|                1|         433|2.6364878963533656|2023-04-21|\n",
      "|及时发现|                1|         433|2.6364878963533656|2023-04-21|\n",
      "|    数据|                1|         433|2.6364878963533656|2023-04-21|\n",
      "|    柔嫩|                1|         433|2.6364878963533656|2023-04-21|\n",
      "|    斑点|                1|         433|2.6364878963533656|2023-04-21|\n",
      "|    介绍|                1|         433|2.6364878963533656|2023-04-21|\n",
      "+--------+-----------------+------------+------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+-----------+\n",
      "|database|      tableName|isTemporary|\n",
      "+--------+---------------+-----------+\n",
      "| default|       item_ods|      false|\n",
      "| default|item_word_count|      false|\n",
      "| default|  item_word_idf|      false|\n",
      "| default|        tag_ods|      false|\n",
      "| default|           test|      false|\n",
      "| default|          test2|      false|\n",
      "| default|       user_ods|      false|\n",
      "|        |item_word_count|       true|\n",
      "|        |  item_word_idf|       true|\n",
      "+--------+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compute IDF\n",
    "item_word_idf = spark.sql('''\n",
    "select\n",
    "    key_word,\n",
    "    count(distinct item_id) as item_num_has_word,\n",
    "    max(a.item_num) as item_num_all,\n",
    "    log10(max(a.item_num)/ count(distinct item_id)) as idf,\n",
    "    max(date) as date\n",
    "from\n",
    "    item_word_count,(\n",
    "        select\n",
    "            count(distinct item_id) as item_num\n",
    "        from\n",
    "            item_word_count\n",
    "    ) as a\n",
    "group by\n",
    "    key_word\n",
    "order by\n",
    "    idf desc\n",
    "''')\n",
    "item_word_idf.createOrReplaceTempView(\"item_word_idf\")\n",
    "item_word_idf.show()\n",
    "item_word_idf.write.mode(\"overwrite\").saveAsTable(\"item_word_idf\")\n",
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fb3d6b-2652-4fd4-ae45-2e9131e6786f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute TF\n",
    "item_word_tf = spark.sql(f'''\n",
    "with item_word_total_num as(\n",
    "    select\n",
    "        item_id,\n",
    "        sum(word_count) as word_total\n",
    "    from\n",
    "        item_word_count\n",
    "    group by\n",
    "        item_id\n",
    "), item_all as(\n",
    "    select\n",
    "        distinct id as item_id\n",
    "    from\n",
    "        item_ods\n",
    "), word_all as(\n",
    "    select\n",
    "        distinct key_word\n",
    "    from\n",
    "        item_word_count\n",
    "), item_word_all as(\n",
    "    select\n",
    "        item_id,\n",
    "        key_word\n",
    "    from\n",
    "        item_all,\n",
    "        word_all\n",
    ")\n",
    "select \n",
    "    a.item_id,\n",
    "    a.key_word,\n",
    "    if(b.item_id is null or c.word_count is null or b.item_id = 0, 0, c.word_count/b.word_total) as tf,\n",
    "    '{date_string}' as date\n",
    "from\n",
    "    item_word_all a\n",
    "left join\n",
    "    item_word_total_num b on a.item_id = b.item_id\n",
    "left join\n",
    "    item_word_count c on a.item_id = c.item_id and a.key_word = c.key_word\n",
    "order by\n",
    "    tf desc\n",
    "''')\n",
    "item_word_tf.createOrReplaceGlobalTempView(\"item_word_tf\")\n",
    "item_word_tf.write.mode(\"overwrite\").saveAsTable(\"item_word_tf\")\n",
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c659e114-ec50-44bc-9a30-cefc3d4dba41",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "\nextraneous input 'is' expecting {')', ','}(line 5, pos 13)\n\n== SQL ==\n\nselect \n    tf.item_id,\n    tf.key_word,\n    if(tf.tf is )\n-------------^^^\nfrom\n    item_word_idf as idf\nleft join\n    item_word_tf as tf on idf.key_word = tf.key_word\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# compute tf-idf\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m item_word_tfidf \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'''\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;43mselect \u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;43m    tf.item_id,\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;43m    tf.key_word,\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;43m    if(tf.tf is )\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;43mfrom\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;43m    item_word_idf as idf\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;43mleft join\u001b[39;49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;43m    item_word_tf as tf on idf.key_word = tf.key_word\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;43m'''\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/app/spark-3.0.1/python/pyspark/sql/session.py:649\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;129m@ignore_unicode_prefix\u001b[39m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;129m@since\u001b[39m(\u001b[38;5;241m2.0\u001b[39m)\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msql\u001b[39m(\u001b[38;5;28mself\u001b[39m, sqlQuery):\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns a :class:`DataFrame` representing the result of the given query.\u001b[39;00m\n\u001b[1;32m    641\u001b[0m \n\u001b[1;32m    642\u001b[0m \u001b[38;5;124;03m    :return: :class:`DataFrame`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;124;03m    [Row(f1=1, f2=u'row1'), Row(f1=2, f2=u'row2'), Row(f1=3, f2=u'row3')]\u001b[39;00m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 649\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped)\n",
      "File \u001b[0;32m/opt/app/spark-3.0.1/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/app/spark-3.0.1/python/pyspark/sql/utils.py:134\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    130\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconverted\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mParseException\u001b[0m: \nextraneous input 'is' expecting {')', ','}(line 5, pos 13)\n\n== SQL ==\n\nselect \n    tf.item_id,\n    tf.key_word,\n    if(tf.tf is )\n-------------^^^\nfrom\n    item_word_idf as idf\nleft join\n    item_word_tf as tf on idf.key_word = tf.key_word\n"
     ]
    }
   ],
   "source": [
    "# compute tf-idf\n",
    "# item_word_tfidf = spark.sql('''\n",
    "# select \n",
    "#     tf.item_id,\n",
    "#     tf.key_word,\n",
    "#     if(tf.tf is )\n",
    "# from\n",
    "#     item_word_idf as idf\n",
    "# left join\n",
    "#     item_word_tf as tf on idf.key_word = tf.key_word\n",
    "# ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ff89eb6-60c2-4ab8-9331-72c9b8f41003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# close spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9862654e-79e1-44b7-8f7f-f0db1ecc2fab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
