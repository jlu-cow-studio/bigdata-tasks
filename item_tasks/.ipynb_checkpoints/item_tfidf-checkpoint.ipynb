{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "371b2f3e-b0ff-40b9-87b3-d8e75e1750da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mysql connection\n",
    "import mysql.connector\n",
    "\n",
    "mydb = mysql.connector.connect(\n",
    "  host=\"cowstudio.wayne-lee.cn\",\n",
    "  user=\"cowstudio\",\n",
    "  password=\"cowstudio_2119\",\n",
    "  database=\"cowstudio\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8033e6f-f283-4190-8e7a-cab88023fe04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/20 09:25:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/04/20 09:25:26 WARN spark.SparkContext: Please ensure that the number of slots available on your executors is limited by the number of cores to task cpus and not another custom resource. If cores is not the limiting resource then dynamic allocation will not work properly!\n"
     ]
    }
   ],
   "source": [
    "# get spark session, 2g mem per executor\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# set python env\n",
    "os.environ['PYSPARK_PYTHON'] = \"/opt/conda3/envs/lab2/bin/python\"\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"tiem_tfidf\") \\\n",
    "    .master(\"spark://node01:10077\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.cores.max\", \"3\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"12\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55015174-77c9-47ab-a7c9-2e847da24a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# import jieba and set stop word set\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "\n",
    "stop_words_rdd = sc.textFile(\"hdfs:///user/spark_temp/stopwords.dat\")\n",
    "stop_words_set = set(stop_words_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfe22ea0-d61d-4ece-a0ea-dbce51d89c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define map functions \n",
    "from datetime import datetime\n",
    "\n",
    "date_string = datetime.today().strftime('%Y-%m-%d')\n",
    "# cut item name and description\n",
    "def cut_name_and_desc(item):\n",
    "    id, name, desc = item\n",
    "    name_cut = set(jieba.cut(name))\n",
    "    name_cut_pure = set(jieba.cut(name)) - stop_words_set\n",
    "    desc_cut = set(jieba.cut(desc))\n",
    "    desc_cut_pure = set(jieba.cut(desc)) - stop_words_set\n",
    "    return (id,name_cut,name_cut_pure,desc_cut,desc_cut_pure)\n",
    "\n",
    "# map item's cut list to word count\n",
    "def to_count(item):\n",
    "    id,name_cut,name_cut_pure,desc_cut,desc_cut_pure = item\n",
    "    for i in name_cut_pure:\n",
    "        yield ((id, i),1)\n",
    "    for i in desc_cut_pure:\n",
    "        yield ((id,i),1)\n",
    "        \n",
    "# trnasfer (id, key), count to id,key,count, date\n",
    "def split_key_set_date(item):\n",
    "    key1,count = item\n",
    "    id,key = key1\n",
    "    date = date_string\n",
    "    return id,key,count,date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43819cd8-4a35-49eb-9119-3f7f537e4a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# get all item data from db, set result to an RDD\n",
    "cur = mydb.cursor()\n",
    "cur.execute(\"SELECT id,name,description from items order by RAND() \")\n",
    "result = cur.fetchall()\n",
    "all_items = sc.parallelize(result)\n",
    "print(all_items.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e0e81cd-90a6-4e3a-a1a8-b771cabc492c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "433\n"
     ]
    }
   ],
   "source": [
    "# cut item's name and description\n",
    "cut_items = all_items.map(cut_name_and_desc)\n",
    "print(all_items.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b665cdb-b2b4-4532-9627-70b63e9a1308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:===================>                                       (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# do word count\n",
    "item_word_count = cut_items.flatMap(to_count)\\\n",
    "                    .reduceByKey(lambda a,b:a+b)\\\n",
    "                    .map(split_key_set_date)\n",
    "print(item_word_count.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84b80ed0-1322-4825-a291-b74594dab776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----------+----------+\n",
      "|item_id|key_word|word_count|      date|\n",
      "+-------+--------+----------+----------+\n",
      "|    113|    黑牛|         2|2023-04-20|\n",
      "|    113|适应能力|         1|2023-04-20|\n",
      "|    113|    适宜|         1|2023-04-20|\n",
      "|    113|东北地区|         1|2023-04-20|\n",
      "|    113|    营养|         1|2023-04-20|\n",
      "|    113|    肉质|         1|2023-04-20|\n",
      "|    113|    品种|         1|2023-04-20|\n",
      "|    113|    生长|         1|2023-04-20|\n",
      "|     89|    牛肉|         2|2023-04-20|\n",
      "|     89|  矿物质|         1|2023-04-20|\n",
      "|     89|    细嫩|         1|2023-04-20|\n",
      "|     89|营养成分|         1|2023-04-20|\n",
      "|     89|味道鲜美|         1|2023-04-20|\n",
      "|    333|    出租|         2|2023-04-20|\n",
      "|    333|    租赁|         1|2023-04-20|\n",
      "|    333|    灵活|         1|2023-04-20|\n",
      "|    333|    服务|         1|2023-04-20|\n",
      "|    371|      牛|         2|2023-04-20|\n",
      "|    371|      质|         1|2023-04-20|\n",
      "|    371|    结实|         1|2023-04-20|\n",
      "+-------+--------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a table for wordcount\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "schema = StructType([\n",
    "    StructField(\"item_id\", IntegerType(), True),\n",
    "    StructField(\"key_word\", StringType(), True),\n",
    "    StructField(\"word_count\", IntegerType(), True),\n",
    "    StructField(\"date\", StringType(),True)\n",
    "])\n",
    "item_word_count = spark.createDataFrame(item_word_count, schema)\n",
    "item_word_count.createOrReplaceTempView(\"item_word_count\")\n",
    "item_word_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1fb03a-6926-4482-920c-f48a520163fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:>                                                         (0 + 3) / 3]\r"
     ]
    }
   ],
   "source": [
    "# 将DataFrame写入MySQL\n",
    "item_word_count.write.format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://cowstudio.wayne-lee.cn:3306/cowstudio\") \\\n",
    "    .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", \"item_word_count\") \\\n",
    "    .option(\"user\", \"cowstudio\") \\\n",
    "    .option(\"password\", \"cowstudio_2119\") \\\n",
    "    .save(mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b221b292-6b30-4d47-b21f-77a05cf6ece7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+------------+------------------+----------+\n",
      "|key_word|item_num_has_word|item_num_all|               idf|      date|\n",
      "+--------+-----------------+------------+------------------+----------+\n",
      "|    吉利|                1|         433|2.6364878963533656|2023-04-20|\n",
      "|    每日|                1|         433|2.6364878963533656|2023-04-20|\n",
      "|    后代|                1|         433|2.6364878963533656|2023-04-20|\n",
      "|      嘴|                1|         433|2.6364878963533656|2023-04-20|\n",
      "|    山地|                1|         433|2.6364878963533656|2023-04-20|\n",
      "|    书籍|                1|         433|2.6364878963533656|2023-04-20|\n",
      "|    弹牙|                1|         433|2.6364878963533656|2023-04-20|\n",
      "|产品质量|                1|         433|2.6364878963533656|2023-04-20|\n",
      "|    强化|                1|         433|2.6364878963533656|2023-04-20|\n",
      "|    优秀|                1|         433|2.6364878963533656|2023-04-20|\n",
      "|    感染|                1|         433|2.6364878963533656|2023-04-20|\n",
      "|    储藏|                1|         433|2.6364878963533656|2023-04-20|\n",
      "|    成分|                1|         433|2.6364878963533656|2023-04-20|\n",
      "|  养牛场|                1|         433|2.6364878963533656|2023-04-20|\n",
      "|    或伴|                1|         433|2.6364878963533656|2023-04-20|\n",
      "|及时发现|                1|         433|2.6364878963533656|2023-04-20|\n",
      "|    数据|                1|         433|2.6364878963533656|2023-04-20|\n",
      "|    柔嫩|                1|         433|2.6364878963533656|2023-04-20|\n",
      "|    斑点|                1|         433|2.6364878963533656|2023-04-20|\n",
      "|    介绍|                1|         433|2.6364878963533656|2023-04-20|\n",
      "+--------+-----------------+------------+------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# compute IDF\n",
    "item_word_idf = spark.sql('''\n",
    "select\n",
    "    key_word,\n",
    "    count(distinct item_id) as item_num_has_word,\n",
    "    max(a.item_num) as item_num_all,\n",
    "    log10(max(a.item_num)/ count(distinct item_id)) as idf,\n",
    "    max(date) as date\n",
    "from\n",
    "    item_word_count,(\n",
    "        select\n",
    "            count(distinct item_id) as item_num\n",
    "        from\n",
    "            item_word_count\n",
    "    ) as a\n",
    "group by\n",
    "    key_word\n",
    "order by\n",
    "    idf desc\n",
    "''')\n",
    "item_word_idf.createOrReplaceTempView(\"item_word_idf\")\n",
    "item_word_idf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77fb3d6b-2652-4fd4-ae45-2e9131e6786f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------------+\n",
      "|item_id|key_word|                 tf|\n",
      "+-------+--------+-------------------+\n",
      "|     14|    牧场|0.10526315789473684|\n",
      "|     14|    夏日|0.10526315789473684|\n",
      "|     14|  松花江|0.10526315789473684|\n",
      "|     14|    繁殖|0.05263157894736842|\n",
      "|     14|    优良|0.05263157894736842|\n",
      "|     14|    观赏|0.05263157894736842|\n",
      "|     14|      月|0.05263157894736842|\n",
      "|     14|    适合|0.05263157894736842|\n",
      "|     14|健康状况|0.05263157894736842|\n",
      "|     14|    家畜|0.05263157894736842|\n",
      "|     14|    怀孕|0.05263157894736842|\n",
      "|     14|    母牛|0.10526315789473684|\n",
      "|     14|      黑|0.10526315789473684|\n",
      "|     14|    这头|0.05263157894736842|\n",
      "|     18|    吉林|0.10526315789473684|\n",
      "|     18|    牛奶|0.05263157894736842|\n",
      "|     18|    养生|0.05263157894736842|\n",
      "|     18|    优质|0.05263157894736842|\n",
      "|     18|    饮用|0.05263157894736842|\n",
      "|     18|    适合|0.05263157894736842|\n",
      "+-------+--------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compute TF\n",
    "item_word_tf = spark.sql('''\n",
    "with item_word_num as(\n",
    "    select\n",
    "        item_id,\n",
    "        sum(word_count) as word_total_count\n",
    "    from\n",
    "        item_word_count\n",
    "    group by\n",
    "        item_id\n",
    ")\n",
    "select \n",
    "    item_word_count.item_id,\n",
    "    item_word_count.key_word,\n",
    "    item_word_count.word_count / item_word_num.word_total_count as tf\n",
    "from\n",
    "    item_word_count\n",
    "left join \n",
    "    item_word_num on item_word_count.item_id = item_word_num.item_id\n",
    "''')\n",
    "item_word_tf.createGlobalTempView(\"item_word_tf\")\n",
    "item_word_tf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c659e114-ec50-44bc-9a30-cefc3d4dba41",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "\nextraneous input 'is' expecting {')', ','}(line 5, pos 13)\n\n== SQL ==\n\nselect \n    tf.item_id,\n    tf.key_word,\n    if(tf.tf is )\n-------------^^^\nfrom\n    item_word_idf as idf\nleft join\n    item_word_tf as tf on idf.key_word = tf.key_word\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# compute tf-idf\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m item_word_tfidf \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'''\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;43mselect \u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;43m    tf.item_id,\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;43m    tf.key_word,\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;43m    if(tf.tf is )\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;43mfrom\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;43m    item_word_idf as idf\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;43mleft join\u001b[39;49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;43m    item_word_tf as tf on idf.key_word = tf.key_word\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;43m'''\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/app/spark-3.0.1/python/pyspark/sql/session.py:649\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;129m@ignore_unicode_prefix\u001b[39m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;129m@since\u001b[39m(\u001b[38;5;241m2.0\u001b[39m)\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msql\u001b[39m(\u001b[38;5;28mself\u001b[39m, sqlQuery):\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns a :class:`DataFrame` representing the result of the given query.\u001b[39;00m\n\u001b[1;32m    641\u001b[0m \n\u001b[1;32m    642\u001b[0m \u001b[38;5;124;03m    :return: :class:`DataFrame`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;124;03m    [Row(f1=1, f2=u'row1'), Row(f1=2, f2=u'row2'), Row(f1=3, f2=u'row3')]\u001b[39;00m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 649\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped)\n",
      "File \u001b[0;32m/opt/app/spark-3.0.1/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/app/spark-3.0.1/python/pyspark/sql/utils.py:134\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    130\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconverted\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mParseException\u001b[0m: \nextraneous input 'is' expecting {')', ','}(line 5, pos 13)\n\n== SQL ==\n\nselect \n    tf.item_id,\n    tf.key_word,\n    if(tf.tf is )\n-------------^^^\nfrom\n    item_word_idf as idf\nleft join\n    item_word_tf as tf on idf.key_word = tf.key_word\n"
     ]
    }
   ],
   "source": [
    "# compute tf-idf\n",
    "# item_word_tfidf = spark.sql('''\n",
    "# select \n",
    "#     tf.item_id,\n",
    "#     tf.key_word,\n",
    "#     if(tf.tf is )\n",
    "# from\n",
    "#     item_word_idf as idf\n",
    "# left join\n",
    "#     item_word_tf as tf on idf.key_word = tf.key_word\n",
    "# ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ff89eb6-60c2-4ab8-9331-72c9b8f41003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# close spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9862654e-79e1-44b7-8f7f-f0db1ecc2fab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
