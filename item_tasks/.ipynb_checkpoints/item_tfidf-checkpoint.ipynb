{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "371b2f3e-b0ff-40b9-87b3-d8e75e1750da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mysql connection\n",
    "import mysql.connector\n",
    "\n",
    "mydb = mysql.connector.connect(\n",
    "  host=\"cowstudio.wayne-lee.cn\",\n",
    "  user=\"cowstudio\",\n",
    "  password=\"cowstudio_2119\",\n",
    "  database=\"cowstudio\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8033e6f-f283-4190-8e7a-cab88023fe04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/21 15:23:55 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/04/21 15:23:56 WARN spark.SparkContext: Please ensure that the number of slots available on your executors is limited by the number of cores to task cpus and not another custom resource. If cores is not the limiting resource then dynamic allocation will not work properly!\n"
     ]
    }
   ],
   "source": [
    "# get spark session, 2g mem per executor\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# set python env\n",
    "os.environ['PYSPARK_PYTHON'] = \"/opt/conda3/envs/lab2/bin/python\"\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"item_tfidf\") \\\n",
    "    .master(\"spark://node01:10077\") \\\n",
    "    .enableHiveSupport()\\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.cores.max\", \"3\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"12\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55015174-77c9-47ab-a7c9-2e847da24a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# import jieba and set stop word set\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "\n",
    "stop_words_rdd = sc.textFile(\"hdfs:///user/spark_temp/stopwords.dat\")\n",
    "stop_words_set = set(stop_words_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfe22ea0-d61d-4ece-a0ea-dbce51d89c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define map functions \n",
    "from datetime import datetime\n",
    "\n",
    "date_string = datetime.today().strftime('%Y-%m-%d')\n",
    "# cut item name and description\n",
    "def cut_name_and_desc(item):\n",
    "    id, name, desc = item\n",
    "    name_cut = set(jieba.cut(name))\n",
    "    name_cut_pure = set(jieba.cut(name)) - stop_words_set\n",
    "    desc_cut = set(jieba.cut(desc))\n",
    "    desc_cut_pure = set(jieba.cut(desc)) - stop_words_set\n",
    "    return (id,name_cut,name_cut_pure,desc_cut,desc_cut_pure)\n",
    "\n",
    "# map item's cut list to word count\n",
    "def to_count(item):\n",
    "    id,name_cut,name_cut_pure,desc_cut,desc_cut_pure = item\n",
    "    for i in name_cut_pure:\n",
    "        yield ((id, i),1)\n",
    "    for i in desc_cut_pure:\n",
    "        yield ((id,i),1)\n",
    "        \n",
    "# trnasfer (id, key), count to id,key,count, date\n",
    "def split_key_set_date(item):\n",
    "    key1,count = item\n",
    "    id,key = key1\n",
    "    date = date_string\n",
    "    return id,key,count,date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43819cd8-4a35-49eb-9119-3f7f537e4a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# get all item data from db, set result to an RDD\n",
    "cur = mydb.cursor()\n",
    "cur.execute(\"SELECT id,name,description from items order by RAND() \")\n",
    "result = cur.fetchall()\n",
    "all_items = sc.parallelize(result)\n",
    "print(all_items.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b47b727-0feb-468b-bf07-ec92ea0fcce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "all_items = spark.sql(\"select id,name,description from item_ods\").rdd\n",
    "print(all_items.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e0e81cd-90a6-4e3a-a1a8-b771cabc492c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "433\n"
     ]
    }
   ],
   "source": [
    "# cut item's name and description\n",
    "cut_items = all_items.map(cut_name_and_desc)\n",
    "print(all_items.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b665cdb-b2b4-4532-9627-70b63e9a1308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# do word count\n",
    "item_word_count = cut_items.flatMap(to_count)\\\n",
    "                    .reduceByKey(lambda a,b:a+b)\\\n",
    "                    .map(split_key_set_date)\n",
    "print(item_word_count.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84b80ed0-1322-4825-a291-b74594dab776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----------+----------+\n",
      "|item_id|key_word|word_count|      date|\n",
      "+-------+--------+----------+----------+\n",
      "|    227|    牛用|         1|2023-04-21|\n",
      "|    227|    饲养|         1|2023-04-21|\n",
      "|    227|      牛|         1|2023-04-21|\n",
      "|    227|多种营养|         1|2023-04-21|\n",
      "|    227|    产地|         1|2023-04-21|\n",
      "|    227|    物质|         1|2023-04-21|\n",
      "|    227|    富含|         1|2023-04-21|\n",
      "|    227|    日常|         1|2023-04-21|\n",
      "|    228|    清凉|         1|2023-04-21|\n",
      "|    228|  驱虫剂|         1|2023-04-21|\n",
      "|    228|    驱虫|         1|2023-04-21|\n",
      "|    228|    降温|         1|2023-04-21|\n",
      "|    228|    蚊虫|         1|2023-04-21|\n",
      "|    228|    叮咬|         1|2023-04-21|\n",
      "|    228|    用于|         1|2023-04-21|\n",
      "|    229|  止咳药|         1|2023-04-21|\n",
      "|    229|    咳喘|         1|2023-04-21|\n",
      "|    229|    牛用|         1|2023-04-21|\n",
      "|    229|    咳嗽|         1|2023-04-21|\n",
      "|    229|      牛|         1|2023-04-21|\n",
      "+-------+--------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a table for wordcount\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "schema = StructType([\n",
    "    StructField(\"item_id\", IntegerType(), True),\n",
    "    StructField(\"key_word\", StringType(), True),\n",
    "    StructField(\"word_count\", IntegerType(), True),\n",
    "    StructField(\"date\", StringType(),True)\n",
    "])\n",
    "item_word_count = spark.createDataFrame(item_word_count, schema)\n",
    "item_word_count.createOrReplaceTempView(\"item_word_count\")\n",
    "item_word_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c1fb03a-6926-4482-920c-f48a520163fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/21 15:24:43 WARN session.SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+-----------+\n",
      "|database|      tableName|isTemporary|\n",
      "+--------+---------------+-----------+\n",
      "| default|       item_ods|      false|\n",
      "| default|item_word_count|      false|\n",
      "| default|  item_word_idf|      false|\n",
      "| default|   item_word_tf|      false|\n",
      "| default|item_word_tfidf|      false|\n",
      "| default|        tag_ods|      false|\n",
      "| default|           test|      false|\n",
      "| default|          test2|      false|\n",
      "| default|       user_ods|      false|\n",
      "|        |item_word_count|       true|\n",
      "+--------+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 将DataFrame写入MySQL\n",
    "item_word_count.write.mode(\"overwrite\").partitionBy(\"date\").saveAsTable(\"item_word_count\")\n",
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b221b292-6b30-4d47-b21f-77a05cf6ece7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+------------+------------------+----------+\n",
      "|key_word|item_num_has_word|item_num_all|               idf|      date|\n",
      "+--------+-----------------+------------+------------------+----------+\n",
      "|    咳嗽|                1|         433|2.6364878963533656|2023-04-21|\n",
      "|    支持|                1|         433|2.6364878963533656|2023-04-21|\n",
      "|    提升|                1|         433|2.6364878963533656|2023-04-21|\n",
      "|      场|                1|         433|2.6364878963533656|2023-04-21|\n",
      "|    一本|                1|         433|2.6364878963533656|2023-04-21|\n",
      "|    场合|                1|         433|2.6364878963533656|2023-04-21|\n",
      "|    人群|                1|         433|2.6364878963533656|2023-04-21|\n",
      "|    女性|                1|         433|2.6364878963533656|2023-04-21|\n",
      "|    全国|                1|         433|2.6364878963533656|2023-04-21|\n",
      "|    宴席|                1|         433|2.6364878963533656|2023-04-21|\n",
      "|    准时|                1|         433|2.6364878963533656|2023-04-21|\n",
      "|    家禽|                1|         433|2.6364878963533656|2023-04-21|\n",
      "|    华南|                1|         433|2.6364878963533656|2023-04-21|\n",
      "|    干草|                1|         433|2.6364878963533656|2023-04-21|\n",
      "|  一对一|                1|         433|2.6364878963533656|2023-04-21|\n",
      "|  广东省|                1|         433|2.6364878963533656|2023-04-21|\n",
      "|    产后|                1|         433|2.6364878963533656|2023-04-21|\n",
      "|    弹嫩|                1|         433|2.6364878963533656|2023-04-21|\n",
      "|  净含量|                1|         433|2.6364878963533656|2023-04-21|\n",
      "|    打针|                1|         433|2.6364878963533656|2023-04-21|\n",
      "+--------+-----------------+------------+------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+---------------+-----------+\n",
      "|database|      tableName|isTemporary|\n",
      "+--------+---------------+-----------+\n",
      "| default|       item_ods|      false|\n",
      "| default|item_word_count|      false|\n",
      "| default|  item_word_idf|      false|\n",
      "| default|   item_word_tf|      false|\n",
      "| default|item_word_tfidf|      false|\n",
      "| default|        tag_ods|      false|\n",
      "| default|           test|      false|\n",
      "| default|          test2|      false|\n",
      "| default|       user_ods|      false|\n",
      "|        |item_word_count|       true|\n",
      "|        |  item_word_idf|       true|\n",
      "+--------+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compute IDF\n",
    "item_word_idf = spark.sql('''\n",
    "select\n",
    "    key_word,\n",
    "    count(distinct item_id) as item_num_has_word,\n",
    "    max(a.item_num) as item_num_all,\n",
    "    log10(max(a.item_num)/ count(distinct item_id)) as idf,\n",
    "    max(date) as date\n",
    "from\n",
    "    item_word_count,(\n",
    "        select\n",
    "            count(distinct item_id) as item_num\n",
    "        from\n",
    "            item_word_count\n",
    "    ) as a\n",
    "group by\n",
    "    key_word\n",
    "order by\n",
    "    idf desc\n",
    "''')\n",
    "item_word_idf.createOrReplaceTempView(\"item_word_idf\")\n",
    "item_word_idf.show()\n",
    "item_word_idf.write.mode(\"overwrite\").saveAsTable(\"item_word_idf\")\n",
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77fb3d6b-2652-4fd4-ae45-2e9131e6786f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+-----------+\n",
      "|database|      tableName|isTemporary|\n",
      "+--------+---------------+-----------+\n",
      "| default|       item_ods|      false|\n",
      "| default|item_word_count|      false|\n",
      "| default|  item_word_idf|      false|\n",
      "| default|   item_word_tf|      false|\n",
      "| default|item_word_tfidf|      false|\n",
      "| default|        tag_ods|      false|\n",
      "| default|           test|      false|\n",
      "| default|          test2|      false|\n",
      "| default|       user_ods|      false|\n",
      "|        |item_word_count|       true|\n",
      "|        |  item_word_idf|       true|\n",
      "+--------+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compute TF\n",
    "item_word_tf = spark.sql(f'''\n",
    "with item_word_total_num as(\n",
    "    select\n",
    "        item_id,\n",
    "        sum(word_count) as word_total\n",
    "    from\n",
    "        item_word_count\n",
    "    group by\n",
    "        item_id\n",
    "), item_all as(\n",
    "    select\n",
    "        distinct id as item_id\n",
    "    from\n",
    "        item_ods\n",
    "), word_all as(\n",
    "    select\n",
    "        distinct key_word\n",
    "    from\n",
    "        item_word_count\n",
    "), item_word_all as(\n",
    "    select\n",
    "        item_id,\n",
    "        key_word\n",
    "    from\n",
    "        item_all,\n",
    "        word_all\n",
    ")\n",
    "select \n",
    "    a.item_id,\n",
    "    a.key_word,\n",
    "    if(b.item_id is null or c.word_count is null or b.item_id = 0, 0, c.word_count/b.word_total) as tf,\n",
    "    '{date_string}' as date\n",
    "from\n",
    "    item_word_all a\n",
    "left join\n",
    "    item_word_total_num b on a.item_id = b.item_id\n",
    "left join\n",
    "    item_word_count c on a.item_id = c.item_id and a.key_word = c.key_word\n",
    "order by\n",
    "    tf desc\n",
    "''')\n",
    "item_word_tf.createOrReplaceGlobalTempView(\"item_word_tf\")\n",
    "item_word_tf.write.mode(\"overwrite\").saveAsTable(\"item_word_tf\")\n",
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c659e114-ec50-44bc-9a30-cefc3d4dba41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------------------+----------+\n",
      "|item_id|key_word|             tfidf|      date|\n",
      "+-------+--------+------------------+----------+\n",
      "|     80|  黑山羊|0.8788292987844551|2023-04-21|\n",
      "|     39|    延边|0.8788292987844551|2023-04-21|\n",
      "|    245|咨询服务|0.8637466566534813|2023-04-21|\n",
      "|    236|    牛类|0.8137711620101613|2023-04-21|\n",
      "|     79|    牛骨|0.7784859668964614|2023-04-21|\n",
      "|     78|    红烧|0.7784859668964614|2023-04-21|\n",
      "|    243|    建设|0.7532822561009616|2023-04-21|\n",
      "|    279|消化不良|0.7532822561009616|2023-04-21|\n",
      "|    242|    乳牛|0.7165559425356435|2023-04-21|\n",
      "|     76|    牛排|0.6781426350084676|2023-04-21|\n",
      "|    225|  抗生素|0.6672736859112526|2023-04-21|\n",
      "|    253|    奶粉|0.6672736859112526|2023-04-21|\n",
      "|    243|    牛棚|0.6672736859112526|2023-04-21|\n",
      "|    280|  胃肠炎|0.6591219740883414|2023-04-21|\n",
      "|    214|    储藏|0.6591219740883414|2023-04-21|\n",
      "|     12|  南犬牛|0.6591219740883414|2023-04-21|\n",
      "|    232|  牛舒安|0.6591219740883414|2023-04-21|\n",
      "|     34|      拉|0.6458392973391155|2023-04-21|\n",
      "|     33|  巴拉巴|0.6458392973391155|2023-04-21|\n",
      "|     36|      拉|0.6458392973391155|2023-04-21|\n",
      "+-------+--------+------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+-----------+\n",
      "|database|      tableName|isTemporary|\n",
      "+--------+---------------+-----------+\n",
      "| default|       item_ods|      false|\n",
      "| default|item_word_count|      false|\n",
      "| default|  item_word_idf|      false|\n",
      "| default|   item_word_tf|      false|\n",
      "| default|item_word_tfidf|      false|\n",
      "| default|        tag_ods|      false|\n",
      "| default|           test|      false|\n",
      "| default|          test2|      false|\n",
      "| default|       user_ods|      false|\n",
      "|        |item_word_count|       true|\n",
      "|        |  item_word_idf|       true|\n",
      "+--------+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compute tf-idf\n",
    "item_word_tfidf = spark.sql('''\n",
    "select \n",
    "    tf.item_id,\n",
    "    tf.key_word,\n",
    "    tf.tf * idf.idf as tfidf,\n",
    "    tf.date\n",
    "from\n",
    "    item_word_tf as tf\n",
    "left join\n",
    "    item_word_idf as idf on idf.key_word = tf.key_word\n",
    "order by\n",
    "    tfidf desc\n",
    "''')\n",
    "item_word_tfidf.createOrReplaceGlobalTempView(\"item_word_tfidf\")\n",
    "item_word_tfidf.show()\n",
    "item_word_tfidf.write.mode(\"overwrite\").saveAsTable(\"item_word_tfidf\")\n",
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ff89eb6-60c2-4ab8-9331-72c9b8f41003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# close spark session\n",
    "spark.stop()\n",
    "mydb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9862654e-79e1-44b7-8f7f-f0db1ecc2fab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
