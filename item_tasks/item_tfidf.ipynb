{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "371b2f3e-b0ff-40b9-87b3-d8e75e1750da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mysql connection\n",
    "import mysql.connector\n",
    "\n",
    "mydb = mysql.connector.connect(\n",
    "  host=\"cowstudio.wayne-lee.cn\",\n",
    "  user=\"cowstudio\",\n",
    "  password=\"cowstudio_2119\",\n",
    "  database=\"cowstudio\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8033e6f-f283-4190-8e7a-cab88023fe04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/19 02:13:07 WARN spark.SparkContext: Please ensure that the number of slots available on your executors is limited by the number of cores to task cpus and not another custom resource. If cores is not the limiting resource then dynamic allocation will not work properly!\n"
     ]
    }
   ],
   "source": [
    "# get spark session, 2g mem per executor\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# set python env\n",
    "os.environ['PYSPARK_PYTHON'] = \"/opt/conda3/envs/lab2/bin/python\"\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"tiem_tfidf\") \\\n",
    "    .master(\"spark://node01:10077\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.cores.max\", \"3\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"12\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55015174-77c9-47ab-a7c9-2e847da24a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# import jieba and set stop word set\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "\n",
    "stop_words_rdd = sc.textFile(\"hdfs:///user/spark_temp/stopwords.dat\")\n",
    "stop_words_set = set(stop_words_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfe22ea0-d61d-4ece-a0ea-dbce51d89c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define map functions \n",
    "from datetime import datetime\n",
    "\n",
    "date_string = datetime.today().strftime('%Y-%m-%d')\n",
    "# cut item name and description\n",
    "def cut_name_and_desc(item):\n",
    "    id, name, desc = item\n",
    "    name_cut = set(jieba.cut(name))\n",
    "    name_cut_pure = set(jieba.cut(name)) - stop_words_set\n",
    "    desc_cut = set(jieba.cut(desc))\n",
    "    desc_cut_pure = set(jieba.cut(desc)) - stop_words_set\n",
    "    return (id,name_cut,name_cut_pure,desc_cut,desc_cut_pure)\n",
    "\n",
    "# map item's cut list to word count\n",
    "def to_count(item):\n",
    "    id,name_cut,name_cut_pure,desc_cut,desc_cut_pure = item\n",
    "    for i in name_cut_pure:\n",
    "        yield ((id, i),1)\n",
    "    for i in desc_cut_pure:\n",
    "        yield ((id,i),1)\n",
    "        \n",
    "# trnasfer (id, key), count to id,key,count, date\n",
    "def split_key_set_date(item):\n",
    "    key1,count = item\n",
    "    id,key = key1\n",
    "    date = date_string\n",
    "    return id,key,count,date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43819cd8-4a35-49eb-9119-3f7f537e4a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# get all item data from db, set result to an RDD\n",
    "cur = mydb.cursor()\n",
    "cur.execute(\"SELECT id,name,description from items order by RAND() \")\n",
    "result = cur.fetchall()\n",
    "all_items = sc.parallelize(result)\n",
    "print(all_items.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e0e81cd-90a6-4e3a-a1a8-b771cabc492c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "433\n"
     ]
    }
   ],
   "source": [
    "# cut item's name and description\n",
    "cut_items = all_items.map(cut_name_and_desc)\n",
    "print(all_items.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b665cdb-b2b4-4532-9627-70b63e9a1308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 3) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# do word count\n",
    "item_word_count = cut_items.flatMap(to_count)\\\n",
    "                    .reduceByKey(lambda a,b:a+b)\\\n",
    "                    .map(split_key_set_date)\n",
    "print(item_word_count.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84b80ed0-1322-4825-a291-b74594dab776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----------+----------+\n",
      "|item_id|key_word|word_count|      date|\n",
      "+-------+--------+----------+----------+\n",
      "|    218|      牛|         2|2023-04-19|\n",
      "|    218|      药|         1|2023-04-19|\n",
      "|    218|    用于|         1|2023-04-19|\n",
      "|     87|    佳品|         1|2023-04-19|\n",
      "|     87|    水分|         1|2023-04-19|\n",
      "|     87|    传统|         1|2023-04-19|\n",
      "|     87|    优质|         1|2023-04-19|\n",
      "|     87|    精选|         1|2023-04-19|\n",
      "|     87|    烹饪|         1|2023-04-19|\n",
      "|     16|      斤|         1|2023-04-19|\n",
      "|     16|    健康|         1|2023-04-19|\n",
      "|     16|    放养|         1|2023-04-19|\n",
      "|     16|    极佳|         1|2023-04-19|\n",
      "|     68|  酸酸乳|         2|2023-04-19|\n",
      "|     68|    原味|         2|2023-04-19|\n",
      "|     68|    蒙牛|         2|2023-04-19|\n",
      "|     68|        |         1|2023-04-19|\n",
      "|     68|    适中|         1|2023-04-19|\n",
      "|     68|    口感|         1|2023-04-19|\n",
      "|     68|    助手|         1|2023-04-19|\n",
      "+-------+--------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a table for wordcount\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "schema = StructType([\n",
    "    StructField(\"item_id\", IntegerType(), True),\n",
    "    StructField(\"key_word\", StringType(), True),\n",
    "    StructField(\"word_count\", IntegerType(), True),\n",
    "    StructField(\"date\", StringType(),True)\n",
    "])\n",
    "item_word_count = spark.createDataFrame(item_word_count, schema)\n",
    "item_word_count.createOrReplaceTempView(\"item_word_count\")\n",
    "item_word_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b221b292-6b30-4d47-b21f-77a05cf6ece7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:=======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+------------+------------------+\n",
      "|key_word|item_num_has_word|item_num_all|               idf|\n",
      "+--------+-----------------+------------+------------------+\n",
      "|    肉质|              101|         433|0.6321665225707229|\n",
      "|    屠宰|               17|         433|1.4060389749750914|\n",
      "|    产后|                1|         433|2.6364878963533656|\n",
      "|    一号|               22|         433|1.2940652155311592|\n",
      "|    体重|                4|         433| 2.034427905025403|\n",
      "|    助手|                1|         433|2.6364878963533656|\n",
      "|      猪|                4|         433| 2.034427905025403|\n",
      "|    品质|               11|         433|1.5950952111951404|\n",
      "|      约|                7|         433|1.7913898563391086|\n",
      "|    放养|                3|         433| 2.159366641633703|\n",
      "|    效益|                3|         433| 2.159366641633703|\n",
      "|    塔尔|                2|         433|2.3354579006893843|\n",
      "|    独特|                4|         433| 2.034427905025403|\n",
      "|    消毒|                2|         433|2.3354579006893843|\n",
      "|    鲜蔬|                1|         433|2.6364878963533656|\n",
      "|    准时|                1|         433|2.6364878963533656|\n",
      "|    鲜奶|                3|         433| 2.159366641633703|\n",
      "|    场地|                7|         433|1.7913898563391086|\n",
      "|    牛奶|               22|         433|1.2940652155311592|\n",
      "|    宴席|                1|         433|2.6364878963533656|\n",
      "+--------+-----------------+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# compute IDF\n",
    "item_word_idf = spark.sql('''\n",
    "select\n",
    "    key_word,\n",
    "    count(distinct item_id) as item_num_has_word,\n",
    "    max(a.item_num) as item_num_all,\n",
    "    log10(max(a.item_num)/ count(distinct item_id)) as idf\n",
    "from\n",
    "    item_word_count,(\n",
    "        select\n",
    "            count(distinct item_id) as item_num\n",
    "        from\n",
    "            item_word_count\n",
    "    ) as a\n",
    "group by\n",
    "    key_word\n",
    "''')\n",
    "item_word_idf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fb3d6b-2652-4fd4-ae45-2e9131e6786f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c659e114-ec50-44bc-9a30-cefc3d4dba41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ff89eb6-60c2-4ab8-9331-72c9b8f41003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# close spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9862654e-79e1-44b7-8f7f-f0db1ecc2fab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
